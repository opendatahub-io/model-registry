source: Third-party
models:
- repository: neuralmagic
  name: granite-3.1-8b-base-quantized.w4a16
  provider: Neural Magic
  description:  Quantized version of ibm-granite/granite-3.1-8b-base.
  longDescription: |-
    Quantized version of ibm-granite/granite-3.1-8b-base. It achieves an average 
      score of 69.81 on the OpenLLM benchmark (version 1), whereas the unquantized 
      model achieves 70.30.
  readme: |-
    # granite-3.1-8b-base-quantized.w4a16

    ## Model Overview
    - **Model Architecture:** granite-3.1-8b-base
      - **Input:** Text
      - **Output:** Text
    - **Model Optimizations:**
      - **Weight quantization:** INT4
      - **Activation quantization:** INT4
    - **Release Date:** 1/8/2025
    - **Version:** 1.0
    - **Model Developers:** Neural Magic

    Quantized version of [ibm-granite/granite-3.1-8b-base](https://huggingface.co/ibm-granite/granite-3.1-8b-base).
    It achieves an average score of 69.81 on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) benchmark (version 1), whereas the unquantized model achieves 70.30.

    ### Model Optimizations

    This model was obtained by quantizing the weights of [ibm-granite/granite-3.1-8b-base](https://huggingface.co/ibm-granite/granite-3.1-8b-base) to INT4 data type, ready for inference with vLLM >= 0.5.2.
    This optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%. Only the weights of the linear operators within transformers blocks are quantized. 

    ## Deployment

    ### Use with vLLM

    This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

    ```python
    from transformers import AutoTokenizer
    from vllm import LLM, SamplingParams
    max_model_len, tp_size = 4096, 1
    model_name = "neuralmagic/granite-3.1-8b-base-quantized.w4a16"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    llm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True)
    sampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])
    messages_list = [
        [{"role": "user", "content": "Who are you? Please respond in pirate speak!"}],
    ]
    prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]
    outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)
    generated_text = [output.outputs[0].text for output in outputs]
    print(generated_text)
    ```

    vLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

    ## Creation

    This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. 

    <details>
      <summary>Model Creation Code</summary>

    ```bash
    python quantize.py --model_path ibm-granite/granite-3.1-8b-base --quant_path "output_dir/granite-3.1-8b-base-quantized.w4a16" --calib_size 3072 --dampening_frac 0.1 --observer mse --actorder static
    ```


    ```python
    from datasets import load_dataset
    from transformers import AutoTokenizer
    from llmcompressor.modifiers.quantization import GPTQModifier
    from llmcompressor.transformers import SparseAutoModelForCausalLM, oneshot, apply
    import argparse
    from compressed_tensors.quantization import QuantizationScheme, QuantizationArgs, QuantizationType, QuantizationStrategy
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_path', type=str)
    parser.add_argument('--quant_path', type=str)
    parser.add_argument('--calib_size', type=int, default=256)
    parser.add_argument('--dampening_frac', type=float, default=0.1) 
    parser.add_argument('--observer', type=str, default="minmax")
    parser.add_argument('--actorder', type=str, default="dynamic")
    args = parser.parse_args()
    model = SparseAutoModelForCausalLM.from_pretrained(
        args.model_path,
        device_map="auto",
        torch_dtype="auto",
        use_cache=False,
        trust_remote_code=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(args.model_path)
    NUM_CALIBRATION_SAMPLES = args.calib_size
    DATASET_ID = "neuralmagic/LLM_compression_calibration"
    DATASET_SPLIT = "train"
    ds = load_dataset(DATASET_ID, split=DATASET_SPLIT)
    ds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))
    def preprocess(example):
        return {"text": example["text"]}
    ds = ds.map(preprocess)
    def tokenize(sample):
        return tokenizer(
            sample["text"],
            padding=False,
            truncation=False,
            add_special_tokens=True,
        )
    ds = ds.map(tokenize, remove_columns=ds.column_names)
    recipe = [
        GPTQModifier(
            targets=["Linear"],
            ignore=["lm_head"],
            scheme="w4a16",
            dampening_frac=args.dampening_frac,
            observer=args.observer,
            actorder=args.actorder,
        )
    ]
    oneshot(
        model=model,
        dataset=ds,
        recipe=recipe,
        num_calibration_samples=args.calib_size,
        max_seq_length=8196,
    )
    # Save to disk compressed.
    model.save_pretrained(quant_path, save_compressed=True)
    tokenizer.save_pretrained(quant_path)
    ```
    </details>

    ## Evaluation

    The model was evaluated on OpenLLM Leaderboard [V1](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard), OpenLLM Leaderboard [V2](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/) and on [HumanEval](https://github.com/neuralmagic/evalplus), using the following commands:

    <details>
    <summary>Evaluation Commands</summary>

    OpenLLM Leaderboard V1:
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/granite-3.1-8b-base-quantized.w4a16",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \
      --tasks openllm \
      --write_out \
      --batch_size auto \
      --output_path output_dir \
      --show_config
    ```

    OpenLLM Leaderboard V2:
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/granite-3.1-8b-base-quantized.w4a16",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \
      --tasks leaderboard \
      --write_out \
      --batch_size auto \
      --output_path output_dir \
      --show_config
    ```

    #### HumanEval
    ##### Generation
    ```
    python3 codegen/generate.py \
      --model neuralmagic/granite-3.1-8b-base-quantized.w4a16 \
      --bs 16 \
      --temperature 0.2 \
      --n_samples 50 \
      --root "." \
      --dataset humaneval
    ```
    ##### Sanitization
    ```
    python3 evalplus/sanitize.py \
      humaneval/neuralmagic--granite-3.1-8b-base-quantized.w4a16_vllm_temp_0.2
    ```
    ##### Evaluation
    ```
    evalplus.evaluate \
      --dataset humaneval \
      --samples humaneval/neuralmagic--granite-3.1-8b-base-quantized.w4a16_vllm_temp_0.2-sanitized
    ```
    </details>

    ### Accuracy

    <table>
      <thead>
        <tr>
          <th>Category</th>
          <th>Metric</th>
          <th>ibm-granite/granite-3.1-8b-base</th>
          <th>neuralmagic/granite-3.1-8b-base-quantized.w4a16</th>
          <th>Recovery (%)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td rowspan="7"><b>OpenLLM V1</b></td>
          <td>ARC-Challenge (Acc-Norm, 25-shot)</td>
          <td>64.68</td>
          <td>62.37</td>
          <td>96.43</td>
        </tr>
        <tr>
          <td>GSM8K (Strict-Match, 5-shot)</td>
          <td>60.88</td>
          <td>54.89</td>
          <td>90.16</td>
        </tr>
        <tr>
          <td>HellaSwag (Acc-Norm, 10-shot)</td>
          <td>83.52</td>
          <td>82.53</td>
          <td>98.81</td>
        </tr>
        <tr>
          <td>MMLU (Acc, 5-shot)</td>
          <td>63.33</td>
          <td>62.78</td>
          <td>99.13</td>
        </tr>
        <tr>
          <td>TruthfulQA (MC2, 0-shot)</td>
          <td>51.33</td>
          <td>51.30</td>
          <td>99.94</td>
        </tr>
        <tr>
          <td>Winogrande (Acc, 5-shot)</td>
          <td>80.90</td>
          <td>79.24</td>
          <td>97.95</td>
        </tr>
        <tr>
          <td><b>Average Score</b></td>
          <td><b>67.44</b></td>
          <td><b>65.52</b></td>
          <td><b>97.15</b></td>
        </tr>
        <tr>
          <td rowspan="2"><b>Coding</b></td>
          <td>HumanEval Pass@1</td>
          <td>44.10</td>
          <td>40.70</td>
          <td><b>92.28</b></td>
        </tr>
      </tbody>
    </table>

    ---


    ## Inference Performance


    This model achieves up to 2.7x speedup in single-stream deployment and up to 1.5x speedup in multi-stream asynchronous deployment, depending on hardware and use-case scenario.
    The following performance benchmarks were conducted with [vLLM](https://docs.vllm.ai/en/latest/) version 0.6.6.post1, and [GuideLLM](https://github.com/neuralmagic/guidellm).

    <details>
    <summary>Benchmarking Command</summary>

    ```
    guidellm --model neuralmagic/granite-3.1-8b-base-quantized.w4a16 --target "http://localhost:8000/v1" --data-type emulated --data "prompt_tokens=<prompt_tokens>,generated_tokens=<generated_tokens>" --max seconds 360 --backend aiohttp_server
    ```

    </details>

    ### Single-stream performance (measured with vLLM version 0.6.6.post1)
    <table>
      <tr>
        <td></td>
        <td></td>
        <td></td>
        <th style="text-align: center;" colspan="7" >Latency (s)</th>
      </tr>
      <tr>
        <th>GPU class</th>
        <th>Model</th>
        <th>Speedup</th>
        <th>Code Completion<br>prefill: 256 tokens<br>decode: 1024 tokens</th>
        <th>Docstring Generation<br>prefill: 768 tokens<br>decode: 128 tokens</th>
        <th>Code Fixing<br>prefill: 1024 tokens<br>decode: 1024 tokens</th>
        <th>RAG<br>prefill: 1024 tokens<br>decode: 128 tokens</th>
        <th>Instruction Following<br>prefill: 256 tokens<br>decode: 128 tokens</th>
        <th>Multi-turn Chat<br>prefill: 512 tokens<br>decode: 256 tokens</th>
        <th>Large Summarization<br>prefill: 4096 tokens<br>decode: 512 tokens</th>
      </tr>
      <tr>
        <td style="vertical-align: middle;" rowspan="3" >A5000</td>
        <td>granite-3.1-8b-base</td>
        <td></td>
        <td>28.3</td>
        <td>3.7</td>
        <td>28.8</td>
        <td>3.8</td>
        <td>3.6</td>
        <td>7.2</td>
        <td>15.7</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w8a8</td>
        <td>1.60</td>
        <td>17.7</td>
        <td>2.3</td>
        <td>18.0</td>
        <td>2.4</td>
        <td>2.2</td>
        <td>4.5</td>
        <td>10.0</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>
        <td>2.61</td>
        <td>10.3</td>
        <td>1.5</td>
        <td>10.7</td>
        <td>1.5</td>
        <td>1.3</td>
        <td>2.7</td>
        <td>6.6</td>
      </tr>
      <tr>
        <td style="vertical-align: middle;" rowspan="3" >A6000</td>
        <td>granite-3.1-8b-base</td>
        <td></td>
        <td>25.8</td>
        <td>3.4</td>
        <td>26.2</td>
        <td>3.4</td>
        <td>3.3</td>
        <td>6.5</td>
        <td>14.2</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w8a8</td>
        <td>1.50</td>
        <td>17.4</td>
        <td>2.3</td>
        <td>16.9</td>
        <td>2.2</td>
        <td>2.2</td>
        <td>4.4</td>
        <td>9.8</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>
        <td>2.48</td>
        <td>10.0</td>
        <td>1.4</td>
        <td>10.4</td>
        <td>1.5</td>
        <td>1.3</td>
        <td>2.5</td>
        <td>6.2</td>
      </tr>
      <tr>
        <td style="vertical-align: middle;" rowspan="3" >A100</td>
        <td>granite-3.1-8b-base</td>
        <td></td>
        <td>13.6</td>
        <td>1.8</td>
        <td>13.7</td>
        <td>1.8</td>
        <td>1.7</td>
        <td>3.4</td>
        <td>7.3</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w8a8</td>
        <td>1.31</td>
        <td>10.4</td>
        <td>1.3</td>
        <td>10.5</td>
        <td>1.4</td>
        <td>1.3</td>
        <td>2.6</td>
        <td>5.6</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>
        <td>1.80</td>
        <td>7.3</td>
        <td>1.0</td>
        <td>7.4</td>
        <td>1.0</td>
        <td>0.9</td>
        <td>1.9</td>
        <td>4.3</td>
      </tr>
      <tr>
        <td style="vertical-align: middle;" rowspan="3" >L40</td>
        <td>granite-3.1-8b-base</td>
        <td></td>
        <td>25.1</td>
        <td>3.2</td>
        <td>25.3</td>
        <td>3.2</td>
        <td>3.2</td>
        <td>6.3</td>
        <td>13.4</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-FP8-dynamic</td>
        <td>1.47</td>
        <td>16.8</td>
        <td>2.2</td>
        <td>17.1</td>
        <td>2.2</td>
        <td>2.1</td>
        <td>4.2</td>
        <td>9.3</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>
        <td>2.72</td>
        <td>8.9</td>
        <td>1.2</td>
        <td>9.2</td>
        <td>1.2</td>
        <td>1.1</td>
        <td>2.3</td>
        <td>5.3</td>
      </tr>
    </table>

    ### Multi-stream asynchronous performance (measured with vLLM version 0.6.6.post1)
    <table>
      <tr>
        <td></td>
        <td></td>
        <td></td>
        <th style="text-align: center;" colspan="7" >Maximum Throughput (Queries per Second)</th>
      </tr>
      <tr>
        <th>GPU class</th>
        <th>Model</th>
        <th>Speedup</th>
        <th>Code Completion<br>prefill: 256 tokens<br>decode: 1024 tokens</th>
        <th>Docstring Generation<br>prefill: 768 tokens<br>decode: 128 tokens</th>
        <th>Code Fixing<br>prefill: 1024 tokens<br>decode: 1024 tokens</th>
        <th>RAG<br>prefill: 1024 tokens<br>decode: 128 tokens</th>
        <th>Instruction Following<br>prefill: 256 tokens<br>decode: 128 tokens</th>
        <th>Multi-turn Chat<br>prefill: 512 tokens<br>decode: 256 tokens</th>
        <th>Large Summarization<br>prefill: 4096 tokens<br>decode: 512 tokens</th>
      </tr>
      <tr>
        <td style="vertical-align: middle;" rowspan="3" >A5000</td>
        <td>granite-3.1-8b-base</td>
        <td></td>
        <td>0.8</td>
        <td>3.1</td>
        <td>0.4</td>
        <td>2.5</td>
        <td>6.7</td>
        <td>2.7</td>
        <td>0.3</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w8a8</td>
        <td>1.71</td>
        <td>1.3</td>
        <td>5.2</td>
        <td>0.9</td>
        <td>4.0</td>
        <td>10.5</td>
        <td>4.4</td>
        <td>0.5</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>
        <td>1.46</td>
        <td>1.3</td>
        <td>3.9</td>
        <td>0.8</td>
        <td>2.9</td>
        <td>8.2</td>
        <td>3.6</td>
        <td>0.5</td>
      </tr>
      <tr>
        <td style="vertical-align: middle;" rowspan="3" >A6000</td>
        <td>granite-3.1-8b-base</td>
        <td></td>
        <td>1.3</td>
        <td>5.1</td>
        <td>0.9</td>
        <td>4.0</td>
        <td>0.3</td>
        <td>4.3</td>
        <td>0.6</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w8a8</td>
        <td>1.39</td>
        <td>1.8</td>
        <td>7.0</td>
        <td>1.3</td>
        <td>5.6</td>
        <td>14.0</td>
        <td>6.3</td>
        <td>0.8</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>
        <td>1.09</td>
        <td>1.9</td>
        <td>4.8</td>
        <td>1.0</td>
        <td>3.8</td>
        <td>10.0</td>
        <td>5.0</td>
        <td>0.6</td>
      </tr>
      <tr>
        <td style="vertical-align: middle;" rowspan="3" >A100</td>
        <td>granite-3.1-8b-base</td>
        <td></td>
        <td>3.1</td>
        <td>10.7</td>
        <td>2.1</td>
        <td>8.5</td>
        <td>20.6</td>
        <td>9.6</td>
        <td>1.4</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w8a8</td>
        <td>1.23</td>
        <td>3.8</td>
        <td>14.2</td>
        <td>2.1</td>
        <td>11.4</td>
        <td>25.9</td>
        <td>12.1</td>
        <td>1.7</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>
        <td>0.96</td>
        <td>3.4</td>
        <td>9.0</td>
        <td>2.6</td>
        <td>7.2</td>
        <td>18.0</td>
        <td>8.8</td>
        <td>1.3</td>
      </tr>
      <tr>
        <td style="vertical-align: middle;" rowspan="3" >L40</td>
        <td>granite-3.1-8b-base</td>
        <td></td>
        <td>1.4</td>
        <td>7.8</td>
        <td>1.1</td>
        <td>6.2</td>
        <td>15.5</td>
        <td>6.0</td>
        <td>0.7</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-FP8-dynamic</td>
        <td>1.12</td>
        <td>2.1</td>
        <td>7.4</td>
        <td>1.3</td>
        <td>5.9</td>
        <td>15.3</td>
        <td>6.9</td>
        <td>0.8</td>
      </tr>
      <tr>
        <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>
        <td>1.29</td>
        <td>2.4</td>
        <td>8.9</td>
        <td>1.4</td>
        <td>7.1</td>
        <td>17.8</td>
        <td>7.8</td>
        <td>1.0</td>
      </tr>
    </table>
  logo: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAACYktHRAAAqo0jMgAAAAlwSFlzAAAAVQAAAFUABu0WZwAAAAd0SU1FB+gMExEDHn1ZgdgAAAABb3JOVAHPoneaAAACbUlEQVRIx93UXUiedRgG8N98nRBN5zrwIDZLpXStT0XWu8JkYM1RLGgfUZQydhA7E+rEYmw5txMN7CDCMSPYYYyCwLCTGQQ528FUNmhK4mt6YH6kNuZcvTvYv2fvl+ZpXSfPdd//+76em+u5/w//W1T73k3j2pw27qY+z228ucg+i06q0mDYsAZVTvnDPkX/3lzgjNsWfBjiTp2B9Vm1ol1BekN+hsCXKtW6igfU6w3ZNzSr9qQH9Sj1ztrvf8ayHYE/blEsTHBCm+1gh2VPrT1B3KBE4GN+0yrP3/g4qki4LG54rQmaDKRETxt3O/LgH9RGM2ZhtwlfZTh0IUsgA3kRe8S3OhwEMaOKccev/kyrbzVnzGu5pM7riXiD2SCdL4ZH1ahR410zXvCWxfsbsSl6/u5T10CF97XqjuSKTZgBWwxpEDMnHmojlEi64mejkoY1pp19oD+wUgu6XDSStT8K/BXcPWbalpSTzSYciKJnfe6sklwe9GsJ7Ib6wMp1+cYvKVavg1cs2AVe8hDYasp5PaZsTal703fOeTiXRKsFpWmS94y67uUoV2/OcRcM5hIoc0txSlxhSaP9lpSjyoqkpG4USaZNFfCqocBiKuWj2aRJTSBuWrkmU17X4XquCV40Fr72T+bS7xzi4Zq1GNFrZy6BbW6pQItLNmedxqN7mobUDzTvC5+IKTNgNeSetzvU1SlUvb4AHyn1te2SUeaQw6DTMRddUre+wKy9EvbncPioI476LPt3lrlj847b622PZeQT6hSrze1DNtpNe88T9vhBB9hjUlKfwo0JcMiPliSci3Zz08ab/2O4Cy0mnnC8jSUfAAAAAElFTkSuQmCC
  language: ["en"]
  license: apache-2.0
  licenseLink: https://www.apache.org/licenses/LICENSE-2.0.txt
  maturity: Generally Available
  libraryName: transformers
  baseModel:
    - repository: ibm-granite
    - name: granite-3.1-8b-base
  labels:
    - inference
    - w4a16
    - int4
    - vllm
  tasks:
    - text-generation
  createTimeSinceEpoch: 1736985600
  lastUpdateTimeSinceEpoch: 1740700800
  artifacts: 
    - protocol: NOT YET IN REGISTRY
      createTimeSinceEpoch: 
      tags: 
      uri: https://huggingface.co/neuralmagic/granite-3.1-8b-base-quantized.w4a16
- repository: neuralmagic-ent
  name: Llama-3.1-8B-Instruct-quantized.w4a16
  provider: Neural Magic
  description:  Intended for commercial and research use in English. Similarly to Meta-Llama-3.1-8B-Instruct, this models is intended for assistant-like chat.
  longDescription: |-
    This model is a quantized version of Meta-Llama-3.1-8B-Instruct. It was evaluated 
      on a several tasks to assess the its quality in comparison to the unquatized model, 
      including multiple-choice, math reasoning, and open-ended text generation. Meta-Llama-3.1-8B-Instruct-quantized.w4a16 
      achieves 93.0% recovery for the Arena-Hard evaluation, 98.9% for OpenLLM v1 (using Meta's prompting when available), 
      96.1% for OpenLLM v2, 99.7% for HumanEval pass@1, and 97.4% for HumanEval+ pass@1.
  readme: |-
    # Meta-Llama-3.1-8B-Instruct-quantized.w4a16

    ## Model Overview
    - **Model Architecture:** Meta-Llama-3
      - **Input:** Text
      - **Output:** Text
    - **Model Optimizations:**
      - **Weight quantization:** INT4
    - **Intended Use Cases:** Intended for commercial and research use in English. Similarly to [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), this models is intended for assistant-like chat.
    - **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English.
    - **Release Date:** 7/26/2024
    - **Version:** 1.0
    - **License(s):** Llama3.1
    - **Model Developers:** Neural Magic

    This model is a quantized version of [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).
    It was evaluated on a several tasks to assess the its quality in comparison to the unquatized model, including multiple-choice, math reasoning, and open-ended text generation.
    Meta-Llama-3.1-8B-Instruct-quantized.w4a16 achieves 93.0% recovery for the Arena-Hard evaluation, 98.9% for OpenLLM v1 (using Meta's prompting when available), 96.1% for OpenLLM v2, 99.7% for HumanEval pass@1, and 97.4% for HumanEval+ pass@1.

    ### Model Optimizations

    This model was obtained by quantizing the weights of [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) to INT4 data type.
    This optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%.

    Only the weights of the linear operators within transformers blocks are quantized. Symmetric per-channel quantization is applied, in which a linear scaling per output dimension maps the INT8 and floating point representations of the quantized weights.
    [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) is used for quantization with 10% damping factor and 768 sequences taken from Neural Magic's [LLM compression calibration dataset](https://huggingface.co/datasets/neuralmagic/LLM_compression_calibration).


    ## Deployment

    This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

    ```python
    from vllm import LLM, SamplingParams
    from transformers import AutoTokenizer

    model_id = "neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"
    number_gpus = 1
    max_model_len = 8192

    sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)

    tokenizer = AutoTokenizer.from_pretrained(model_id)

    messages = [
        {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
        {"role": "user", "content": "Who are you?"},
    ]

    prompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)

    llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len)

    outputs = llm.generate(prompts, sampling_params)

    generated_text = outputs[0].outputs[0].text
    print(generated_text)
    ```

    vLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.


    ## Creation

    This model was created by applying the [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) library as presented in the code snipet below.
    Although AutoGPTQ was used for this particular model, Neural Magic is transitioning to using [llm-compressor](https://github.com/vllm-project/llm-compressor) which supports several quantization schemes and models not supported by AutoGPTQ.

    ```python
    from transformers import AutoTokenizer
    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
    from datasets import load_dataset

    model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"

    num_samples = 756
    max_seq_len = 4064

    tokenizer = AutoTokenizer.from_pretrained(model_id)

    def preprocess_fn(example):
      return {"text": tokenizer.apply_chat_template(example["messages"], add_generation_prompt=False, tokenize=False)}

    ds = load_dataset("neuralmagic/LLM_compression_calibration", split="train")
    ds = ds.shuffle().select(range(num_samples))
    ds = ds.map(preprocess_fn)

    examples = [tokenizer(example["text"], padding=False, max_length=max_seq_len, truncation=True) for example in ds]
        
    quantize_config = BaseQuantizeConfig(
      bits=4,
      group_size=128,
      desc_act=True,
      model_file_base_name="model",
      damp_percent=0.1,
    )

    model = AutoGPTQForCausalLM.from_pretrained(
      model_id,
      quantize_config,
      device_map="auto",
    )

    model.quantize(examples)
    model.save_pretrained("Meta-Llama-3.1-8B-Instruct-quantized.w4a16")
    ```

    ## Evaluation

    This model was evaluated on the well-known Arena-Hard, OpenLLM v1, OpenLLM v2, HumanEval, and HumanEval+ benchmarks.
    In all cases, model outputs were generated with the [vLLM](https://docs.vllm.ai/en/stable/) engine.

    Arena-Hard evaluations were conducted using the [Arena-Hard-Auto](https://github.com/lmarena/arena-hard-auto) repository.
    The model generated a single answer for each prompt form Arena-Hard, and each answer was judged twice by GPT-4.
    We report below the scores obtained in each judgement and the average.

    OpenLLM v1 and v2 evaluations were conducted using Neural Magic's fork of [lm-evaluation-harness](https://github.com/neuralmagic/lm-evaluation-harness/tree/llama_3.1_instruct) (branch llama_3.1_instruct).
    This version of the lm-evaluation-harness includes versions of MMLU, ARC-Challenge and GSM-8K that match the prompting style of [Meta-Llama-3.1-Instruct-evals](https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-Instruct-evals) and a few fixes to OpenLLM v2 tasks.

    HumanEval and HumanEval+ evaluations were conducted using Neural Magic's fork of the [EvalPlus](https://github.com/neuralmagic/evalplus) repository.

    Detailed model outputs are available as HuggingFace datasets for [Arena-Hard](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-arena-hard-evals), [OpenLLM v2](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-leaderboard-v2-evals), and [HumanEval](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-humaneval-evals).

    **Note:** Results have been updated after Meta modified the chat template.

    ### Accuracy

    <table>
      <tr>
      <td><strong>Category</strong>
      </td>
      <td><strong>Benchmark</strong>
      </td>
      <td><strong>Meta-Llama-3.1-8B-Instruct </strong>
      </td>
      <td><strong>Meta-Llama-3.1-8B-Instruct-quantized.w4a16 (this model)</strong>
      </td>
      <td><strong>Recovery</strong>
      </td>
      </tr>
      <tr>
      <td rowspan="1" ><strong>LLM as a judge</strong>
      </td>    
      <td>Arena Hard
      </td>
      <td>25.8 (25.1 / 26.5)
      </td>
      <td>27.2 (27.6 / 26.7)
      </td>
      <td>105.4%
      </td>
      </tr>
      <tr>
      <td rowspan="8" ><strong>OpenLLM v1</strong>
      </td>
      <td>MMLU (5-shot)
      </td>
      <td>68.3
      </td>
      <td>66.9
      </td>
      <td>97.9%
      </td>
      </tr>
      <tr>
      <td>MMLU (CoT, 0-shot)
      </td>
      <td>72.8
      </td>
      <td>71.1
      </td>
      <td>97.6%
      </td>
      </tr>
      <tr>
      <td>ARC Challenge (0-shot)
      </td>
      <td>81.4
      </td>
      <td>80.2
      </td>
      <td>98.0%
      </td>
      </tr>
      <tr>
      <td>GSM-8K (CoT, 8-shot, strict-match)
      </td>
      <td>82.8
      </td>
      <td>82.9
      </td>
      <td>100.2%
      </td>
      </tr>
      <tr>
      <td>Hellaswag (10-shot)
      </td>
      <td>80.5
      </td>
      <td>79.9
      </td>
      <td>99.3%
      </td>
      </tr>
      <tr>
      <td>Winogrande (5-shot)
      </td>
      <td>78.1
      </td>
      <td>78.0
      </td>
      <td>99.9%
      </td>
      </tr>
      <tr>
      <td>TruthfulQA (0-shot, mc2)
      </td>
      <td>54.5
      </td>
      <td>52.8
      </td>
      <td>96.9%
      </td>
      </tr>
      <tr>
      <td><strong>Average</strong>
      </td>
      <td><strong>74.3</strong>
      </td>
      <td><strong>73.5</strong>
      </td>
      <td><strong>98.9%</strong>
      </td>
      </tr>
      <tr>
      <td rowspan="7" ><strong>OpenLLM v2</strong>
      </td>
      <td>MMLU-Pro (5-shot)
      </td>
      <td>30.8
      </td>
      <td>28.8
      </td>
      <td>93.6%
      </td>
      </tr>
      <tr>
      <td>IFEval (0-shot)
      </td>
      <td>77.9
      </td>
      <td>76.3
      </td>
      <td>98.0%
      </td>
      </tr>
      <tr>
      <td>BBH (3-shot)
      </td>
      <td>30.1
      </td>
      <td>28.9
      </td>
      <td>96.1%
      </td>
      </tr>
      <tr>
      <td>Math-lvl-5 (4-shot)
      </td>
      <td>15.7
      </td>
      <td>14.8
      </td>
      <td>94.4%
      </td>
      </tr>
      <tr>
      <td>GPQA (0-shot)
      </td>
      <td>3.7
      </td>
      <td>4.0
      </td>
      <td>109.8%
      </td>
      </tr>
      <tr>
      <td>MuSR (0-shot)
      </td>
      <td>7.6
      </td>
      <td>6.3
      </td>
      <td>83.2%
      </td>
      </tr>
      <tr>
      <td><strong>Average</strong>
      </td>
      <td><strong>27.6</strong>
      </td>
      <td><strong>26.5</strong>
      </td>
      <td><strong>96.1%</strong>
      </td>
      </tr>
      <tr>
      <td rowspan="2" ><strong>Coding</strong>
      </td>
      <td>HumanEval pass@1
      </td>
      <td>67.3
      </td>
      <td>67.1
      </td>
      <td>99.7%
      </td>
      </tr>
      <tr>
      <td>HumanEval+ pass@1
      </td>
      <td>60.7
      </td>
      <td>59.1
      </td>
      <td>97.4%
      </td>
      </tr>
      <tr>
      <td rowspan="9" ><strong>Multilingual</strong>
      </td>
      <td>Portuguese MMLU (5-shot)
      </td>
      <td>59.96
      </td>
      <td>58.69
      </td>
      <td>97.9%
      </td>
      </tr>
      <tr>
      <td>Spanish MMLU (5-shot)
      </td>
      <td>60.25
      </td>
      <td>58.39
      </td>
      <td>96.9%
      </td>
      </tr>
      <tr>
      <td>Italian MMLU (5-shot)
      </td>
      <td>59.23
      </td>
      <td>57.82
      </td>
      <td>97.6%
      </td>
      </tr>
      <tr>
      <td>German MMLU (5-shot)
      </td>
      <td>58.63
      </td>
      <td>56.22
      </td>
      <td>95.9%
      </td>
      </tr>
      <tr>
      <td>French MMLU (5-shot)
      </td>
      <td>59.65
      </td>
      <td>57.58
      </td>
      <td>96.5%
      </td>
      </tr>
      <tr>
      <td>Hindi MMLU (5-shot)
      </td>
      <td>50.10
      </td>
      <td>47.14
      </td>
      <td>94.1%
      </td>
      </tr>
      <tr>
      <td>Thai MMLU (5-shot)
      </td>
      <td>49.12
      </td>
      <td>46.72
      </td>
      <td>95.1%
      </td>
      </tr>
    </table>


    ### Reproduction

    The results were obtained using the following commands:

    #### MMLU
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
      --tasks mmlu_llama_3.1_instruct \
      --fewshot_as_multiturn \
      --apply_chat_template \
      --num_fewshot 5 \
      --batch_size auto
    ```

    #### MMLU-CoT
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=4064,max_gen_toks=1024,tensor_parallel_size=1 \
      --tasks mmlu_cot_0shot_llama_3.1_instruct \
      --apply_chat_template \
      --num_fewshot 0 \
      --batch_size auto
    ```

    #### ARC-Challenge
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=3940,max_gen_toks=100,tensor_parallel_size=1 \
      --tasks arc_challenge_llama_3.1_instruct \
      --apply_chat_template \
      --num_fewshot 0 \
      --batch_size auto
    ```

    #### GSM-8K
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=4096,max_gen_toks=1024,tensor_parallel_size=1 \
      --tasks gsm8k_cot_llama_3.1_instruct \
      --fewshot_as_multiturn \
      --apply_chat_template \
      --num_fewshot 8 \
      --batch_size auto
    ```

    #### Hellaswag
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
      --tasks hellaswag \
      --num_fewshot 10 \
      --batch_size auto
    ```

    #### Winogrande
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
      --tasks winogrande \
      --num_fewshot 5 \
      --batch_size auto
    ```

    #### TruthfulQA
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
      --tasks truthfulqa \
      --num_fewshot 0 \
      --batch_size auto
    ```

    #### OpenLLM v2
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=4096,tensor_parallel_size=1,enable_chunked_prefill=True \
      --apply_chat_template \
      --fewshot_as_multiturn \
      --tasks leaderboard \
      --batch_size auto
    ```

    #### MMLU Portuguese
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
      --tasks mmlu_pt_llama_3.1_instruct \
      --fewshot_as_multiturn \
      --apply_chat_template \
      --num_fewshot 5 \
      --batch_size auto
    ```

    #### MMLU Spanish
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
      --tasks mmlu_es_llama_3.1_instruct \
      --fewshot_as_multiturn \
      --apply_chat_template \
      --num_fewshot 5 \
      --batch_size auto
    ```

    #### MMLU Italian
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
      --tasks mmlu_it_llama_3.1_instruct \
      --fewshot_as_multiturn \
      --apply_chat_template \
      --num_fewshot 5 \
      --batch_size auto
    ```

    #### MMLU German
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
      --tasks mmlu_de_llama_3.1_instruct \
      --fewshot_as_multiturn \
      --apply_chat_template \
      --num_fewshot 5 \
      --batch_size auto
    ```

    #### MMLU French
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
      --tasks mmlu_fr_llama_3.1_instruct \
      --fewshot_as_multiturn \
      --apply_chat_template \
      --num_fewshot 5 \
      --batch_size auto
    ```

    #### MMLU Hindi
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
      --tasks mmlu_hi_llama_3.1_instruct \
      --fewshot_as_multiturn \
      --apply_chat_template \
      --num_fewshot 5 \
      --batch_size auto
    ```

    #### MMLU Thai
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
      --tasks mmlu_th_llama_3.1_instruct \
      --fewshot_as_multiturn \
      --apply_chat_template \
      --num_fewshot 5 \
      --batch_size auto
    ```

    #### HumanEval and HumanEval+
    ##### Generation
    ```
    python3 codegen/generate.py \
      --model neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16 \
      --bs 16 \
      --temperature 0.2 \
      --n_samples 50 \
      --root "." \
      --dataset humaneval
    ```
    ##### Sanitization
    ```
    python3 evalplus/sanitize.py \
      humaneval/neuralmagic--Meta-Llama-3.1-8B-Instruct-quantized.w4a16_vllm_temp_0.2
    ```
    ##### Evaluation
    ```
    evalplus.evaluate \
      --dataset humaneval \
      --samples humaneval/neuralmagic--Meta-Llama-3.1-8B-Instruct-quantized.w4a16_vllm_temp_0.2-sanitized
    ```
  logo: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAACYktHRAAAqo0jMgAAAAlwSFlzAAAAVQAAAFUABu0WZwAAAAd0SU1FB+gMExEDHn1ZgdgAAAABb3JOVAHPoneaAAACbUlEQVRIx93UXUiedRgG8N98nRBN5zrwIDZLpXStT0XWu8JkYM1RLGgfUZQydhA7E+rEYmw5txMN7CDCMSPYYYyCwLCTGQQ528FUNmhK4mt6YH6kNuZcvTvYv2fvl+ZpXSfPdd//+76em+u5/w//W1T73k3j2pw27qY+z228ucg+i06q0mDYsAZVTvnDPkX/3lzgjNsWfBjiTp2B9Vm1ol1BekN+hsCXKtW6igfU6w3ZNzSr9qQH9Sj1ztrvf8ayHYE/blEsTHBCm+1gh2VPrT1B3KBE4GN+0yrP3/g4qki4LG54rQmaDKRETxt3O/LgH9RGM2ZhtwlfZTh0IUsgA3kRe8S3OhwEMaOKccev/kyrbzVnzGu5pM7riXiD2SCdL4ZH1ahR410zXvCWxfsbsSl6/u5T10CF97XqjuSKTZgBWwxpEDMnHmojlEi64mejkoY1pp19oD+wUgu6XDSStT8K/BXcPWbalpSTzSYciKJnfe6sklwe9GsJ7Ib6wMp1+cYvKVavg1cs2AVe8hDYasp5PaZsTal703fOeTiXRKsFpWmS94y67uUoV2/OcRcM5hIoc0txSlxhSaP9lpSjyoqkpG4USaZNFfCqocBiKuWj2aRJTSBuWrkmU17X4XquCV40Fr72T+bS7xzi4Zq1GNFrZy6BbW6pQItLNmedxqN7mobUDzTvC5+IKTNgNeSetzvU1SlUvb4AHyn1te2SUeaQw6DTMRddUre+wKy9EvbncPioI476LPt3lrlj847b622PZeQT6hSrze1DNtpNe88T9vhBB9hjUlKfwo0JcMiPliSci3Zz08ab/2O4Cy0mnnC8jSUfAAAAAElFTkSuQmCC
  language:  ["en", "de", "fr", "it", "pt", "hi", "es", "th"]
  license: llama3.1
  licenseLink: https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct/blob/main/LICENSE
  maturity: Generally Available
  libraryName: transformers
  baseModel:
    - repository: meta-llama
    - name: Llama-3.1-8B
  labels:
    - inference
    - int4
    - vllm
  tasks:
    - text-generation
  createTimeSinceEpoch: 1733788800
  lastUpdateTimeSinceEpoch: 1740700800
  artifacts:
    - protocol: NOT YET IN REGISTRY
      createTimeSinceEpoch: 
      tags: 
      uri: https://huggingface.co/neuralmagic-ent/Llama-3.1-8B-Instruct-quantized.w4a16
- repository: neuralmagic-ent
  name: Qwen2.5-7B-Instruct-quantized.w8a8
  provider: Neural Magic
  description: Intended for commercial and research use multiple languages. Similarly to Qwen2.5-7B-Instruct, this models is intended for assistant-like chat.
  longDescription: |-
    Quantized version of Qwen2.5-7B-Instruct. It achieves an average score of 73.05 on the 
      OpenLLM benchmark version 1 and 41.44 on version 2, whereas the unquantized model achieves 
        73.16 on version 1 and 41.40 on version 2.
  readme: |-
    # Qwen2.5-7B-Instruct-quantized.w8a8

    ## Model Overview
    - **Model Architecture:** Qwen2
      - **Input:** Text
      - **Output:** Text
    - **Model Optimizations:**
      - **Activation quantization:** INT8
      - **Weight quantization:** INT8
    - **Intended Use Cases:** Intended for commercial and research use multiple languages. Similarly to [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct), this models is intended for assistant-like chat.
    - **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws).
    - **Release Date:** 10/09/2024
    - **Version:** 1.0
    - **Model Developers:** Neural Magic

    Quantized version of [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct).
    It achieves an average score of 73.05 on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) benchmark version 1 and 41.44 on version 2, whereas the unquantized model achieves 73.16 on version 1 and 41.40 on version 2.

    ### Model Optimizations

    This model was obtained by quantizing the weights and activations of [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) to INT8 data type.
    This optimization reduces the number of bits used to represent weights and activations from 16 to 8, reducing GPU memory requirements (by approximately 50%) and increasing matrix-multiply compute throughput (by approximately 2x).
    Weight quantization also reduces disk size requirements by approximately 50%.

    Only weights and activations of the linear operators within transformers blocks are quantized.
    Weights are quantized with a symmetric static per-channel scheme, where a fixed linear scaling factor is applied between INT8 and floating point representations for each output channel dimension.
    Activations are quantized with a symmetric dynamic per-token scheme, computing a linear scaling factor at runtime for each token between INT8 and floating point representations.

    ## Deployment

    This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

    ```python
    from vllm import LLM, SamplingParams
    from transformers import AutoTokenizer
    model_id = "neuralmagic-ent/Qwen2.5-7B-Instruct-quantized.w8a8"
    number_gpus = 1
    max_model_len = 8192
    sampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    prompt = "Give me a short introduction to large language model."
    llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len)
    outputs = llm.generate(prompt, sampling_params)
    generated_text = outputs[0].outputs[0].text
    print(generated_text)
    ```

    vLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.


    ## Evaluation

    The model was evaluated on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) leaderboard tasks (version 1) with the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/387Bbd54bc621086e05aa1b030d8d4d5635b25e6) (commit 387Bbd54bc621086e05aa1b030d8d4d5635b25e6) and the [vLLM](https://docs.vllm.ai/en/stable/) engine, using the following command:
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic-ent/Qwen2.5-7B-Instruct-quantized.w8a8",dtype=auto,gpu_memory_utilization=0.9,add_bos_token=True,max_model_len=4096,enable_chunk_prefill=True,tensor_parallel_size=1 \
      --tasks openllm \
      --batch_size auto
    ```

    ### Accuracy

    <table>
      <tr>
      <td><strong>Benchmark</strong>
      </td>
      <td><strong>Qwen2.5-7B-Instruct</strong>
      </td>
      <td><strong>Qwen2.5-7B-Instruct-quantized.w8a8 (this model)</strong>
      </td>
      <td><strong>Recovery</strong>
      </td>
      </tr>
      <tr>
      <td rowspan="7" ><strong>OpenLLM v1</strong>
      </td>
      <td>MMLU (5-shot)
      </td>
      <td>74.24
      </td>
      <td>73.84
      </td>
      <td>99.5%
      </td>
      </tr>
      <tr>
      <td>ARC Challenge (25-shot)
      </td>
      <td>63.40
      </td>
      <td>63.23
      </td>
      <td>99.7%
      </td>
      </tr>
      <tr>
      <td>GSM-8K (5-shot, strict-match)
      </td>
      <td>80.36
      </td>
      <td>80.74
      </td>
      <td>100.5%
      </td>
      </tr>
      <tr>
      <td>Hellaswag (10-shot)
      </td>
      <td>81.52
      </td>
      <td>81.06
      </td>
      <td>99.4%
      </td>
      </tr>
      <tr>
      <td>Winogrande (5-shot)
      </td>
      <td>74.66
      </td>
      <td>74.82
      </td>
      <td>100.2%
      </td>
      </tr>
      <tr>
      <td>TruthfulQA (0-shot, mc2)
      </td>
      <td>64.76
      </td>
      <td>64.58
      </td>
      <td>99.7%
      </td>
      </tr>
      <tr>
      <td><strong>Average</strong>
      </td>
      <td><strong>73.16</strong>
      </td>
      <td><strong>73.05</strong>
      </td>
      <td><strong>99.9%</strong>
      </td>
      </tr>
      <tr>
      <td rowspan="7" ><strong>OpenLLM v2</strong>
      </td>
      <td>MMLU-Pro (5-shot)
      </td>
      <td>42.93
      </td>
      <td>42.40
      </td>
      <td>98.8%
      </td>
      </tr>
      <tr>
      <td>IFEval (0-shot)
      </td>
      <td>76.25
      </td>
      <td>75.30
      </td>
      <td>98.8%
      </td>
      </tr>
      <tr>
      <td>BBH (3-shot)
      </td>
      <td>55.56
      </td>
      <td>55.03
      </td>
      <td>99.1%
      </td>
      </tr>
      <tr>
      <td>Math-lvl-5 (4-shot)
      </td>
      <td>0.00
      </td>
      <td>0.00
      </td>
      <td>***
      </td>
      </tr>
      <tr>
      <td>GPQA (0-shot)
      </td>
      <td>33.07
      </td>
      <td>33.74
      </td>
      <td>102.3%
      </td>
      </tr>
      <tr>
      <td>MuSR (0-shot)
      </td>
      <td>40.60
      </td>
      <td>42.18
      </td>
      <td>103.9%
      </td>
      </tr>
      <tr>
      <td><strong>Average</strong>
      </td>
      <td><strong>41.40</strong>
      </td>
      <td><strong>41.44</strong>
      </td>
      <td><strong>100.1%</strong>
      </td>
      </tr>
    </table>
    *** Reference value too low to report meaningful recovery.
  logo: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAACYktHRAAAqo0jMgAAAAlwSFlzAAAAVQAAAFUABu0WZwAAAAd0SU1FB+gMExEDHn1ZgdgAAAABb3JOVAHPoneaAAACbUlEQVRIx93UXUiedRgG8N98nRBN5zrwIDZLpXStT0XWu8JkYM1RLGgfUZQydhA7E+rEYmw5txMN7CDCMSPYYYyCwLCTGQQ528FUNmhK4mt6YH6kNuZcvTvYv2fvl+ZpXSfPdd//+76em+u5/w//W1T73k3j2pw27qY+z228ucg+i06q0mDYsAZVTvnDPkX/3lzgjNsWfBjiTp2B9Vm1ol1BekN+hsCXKtW6igfU6w3ZNzSr9qQH9Sj1ztrvf8ayHYE/blEsTHBCm+1gh2VPrT1B3KBE4GN+0yrP3/g4qki4LG54rQmaDKRETxt3O/LgH9RGM2ZhtwlfZTh0IUsgA3kRe8S3OhwEMaOKccev/kyrbzVnzGu5pM7riXiD2SCdL4ZH1ahR410zXvCWxfsbsSl6/u5T10CF97XqjuSKTZgBWwxpEDMnHmojlEi64mejkoY1pp19oD+wUgu6XDSStT8K/BXcPWbalpSTzSYciKJnfe6sklwe9GsJ7Ib6wMp1+cYvKVavg1cs2AVe8hDYasp5PaZsTal703fOeTiXRKsFpWmS94y67uUoV2/OcRcM5hIoc0txSlxhSaP9lpSjyoqkpG4USaZNFfCqocBiKuWj2aRJTSBuWrkmU17X4XquCV40Fr72T+bS7xzi4Zq1GNFrZy6BbW6pQItLNmedxqN7mobUDzTvC5+IKTNgNeSetzvU1SlUvb4AHyn1te2SUeaQw6DTMRddUre+wKy9EvbncPioI476LPt3lrlj847b622PZeQT6hSrze1DNtpNe88T9vhBB9hjUlKfwo0JcMiPliSci3Zz08ab/2O4Cy0mnnC8jSUfAAAAAElFTkSuQmCC
  language: ["en"]
  license: apache-2.0
  licenseLink: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE
  maturity: Generally Available
  libraryName: transformers
  baseModel:
    - repository: Qwen
    - name: Qwen2.5-7B-Instruct
  labels:
    - inference
    - chat
    - neuralmagic
    - llmcompressor
  tasks:
    - text-generation
  createTimeSinceEpoch: 1733702400
  lastUpdateTimeSinceEpoch: 1740700800
  artifacts:
    - protocol: NOT YET IN REGISTRY
      createTimeSinceEpoch: 
      tags: 
      uri: https://huggingface.co/neuralmagic-ent/Qwen2.5-7B-Instruct-quantized.w8a8
- repository: neuralmagic-ent
  name: Mistral-Small-24B-Instruct-2501-FP8-dynamic
  provider: Neural Magic
  description: This model was obtained by quantizing the weights and activations of Mistral-Small-24B-Instruct-2501 to FP8 data type. 
  longDescription: |-
    This model was obtained by quantizing the weights and activations of Mistral-Small-24B-Instruct-2501 to 
      FP8 data type. This optimization reduces the number of bits per parameter from 16 to 8, reducing the 
      disk size and GPU memory requirements by approximately 50%. Moreover, quantized operators can achieve 2x 
      more FLOPs on supported architectures.

    Only the weights and activations of the linear operators within transformers blocks are quantized. Activations 
      are quantized using a symmetric per-tensor (dynamic) scheme, whereas weights are quantized using a symmetric 
      per-channel scheme. The quantized model is obtained with the llm-compressor library.
  readme: |-
    # Mistral-Small-24B-Instruct-FP8-dynamic

    ## Model Overview
    - **Model Architecture:** MistralForCausalLM
      - **Input:** Text
      - **Output:** Text
    - **Model Optimizations:**
      - **Activation quantization:** FP8
      - **Weight quantization:** FP8
    - **Release Date:** 03/03/2025
    - **Version:** 1.0
    - **Model Developers:** RedHat (Neural Magic)


    ### Model Optimizations


    This model was obtained by quantizing the weights and activations of [Mistral-Small-24B-Instruct-2501](https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501) to FP8 data type.
    This optimization reduces the number of bits per parameter from 16 to 8, reducing the disk size and GPU memory requirements by approximately 50%.
    Moreover, quantized operators can achieve 2x more FLOPs on supported architectures.

    Only the weights and activations of the linear operators within transformers blocks are quantized.
    Activations are quantized using a symmetric per-tensor (dynamic) scheme, whereas weights are quantized using a symmetric per-channel scheme.
    The quantized model is obtained with the [llm-compressor](https://github.com/vllm-project/llm-compressor) library.


    ## Deployment

    This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

    ```python
    from vllm import LLM, SamplingParams
    from transformers import AutoTokenizer

    model_id = "neuralmagic-ent/Mistral-Small-24B-Instruct-FP8-dynamic"
    number_gpus = 1

    sampling_params = SamplingParams(temperature=0.05, top_p=0.9, max_tokens=256)

    tokenizer = AutoTokenizer.from_pretrained(model_id)

    prompt = "Give me a short introduction to large language model."

    llm = LLM(model=model_id, tensor_parallel_size=number_gpus)

    outputs = llm.generate(prompt, sampling_params)

    generated_text = outputs[0].outputs[0].text
    print(generated_text)
    ```

    vLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

    ## Creation

    <details>
      <summary>Creation details</summary>
      This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. 


      ```python
      from transformers import AutoModelForCausalLM
      from llmcompressor.modifiers.quantization import QuantizationModifier
      from llmcompressor.transformers import oneshot
      
      # Load model
      model_stub = "mistralai/Mistral-Small-24B-Instruct-2501"
      model_name = model_stub.split("/")[-1]
        
      model = AutoModelForCausalLM.from_pretrained(
          model_stub,
          device_map="auto",
          torch_dtype="auto",
      )
      
      # Configure the quantization algorithm and scheme
      recipe = QuantizationModifier(
          targets="Linear",
          scheme="FP8_DYNAMIC",
          ignore=["lm_head"],
      )
      
      # Apply quantization
      oneshot(model=model, recipe=recipe)
      
      # Save to disk in compressed-tensors format
      save_path = model_name + "-FP8-dynamic
      model.save_pretrained(save_path)
      tokenizer.save_pretrained(save_path)
      print(f"Model and tokenizer saved to: {save_path}")
      ```
    </details>
    


    ## Evaluation

    The model was evaluated on the OpenLLM leaderboard tasks (version 1) with the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) and the [vLLM](https://docs.vllm.ai/en/stable/) engine, using the following command:
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic-ent/Mistral-Small-24B-Instruct-2501-FP8-dynamic",dtype=auto,gpu_memory_utilization=0.6,max_model_len=4096,enable_chunk_prefill=True,tensor_parallel_size=1 \
      --tasks openllm \
      --batch_size auto
    ```

    ### Accuracy

    #### Open LLM Leaderboard evaluation scores
    <table>
      <tr>
      <td><strong>Benchmark</strong>
      </td>
      <td><strong>Mistral-Small-24B-Instruct-2501</strong>
      </td>
      <td><strong>Mistral-Small-24B-Instruct-2501-FP8-dynamic<br>(this model)</strong>
      </td>
      <td><strong>Recovery</strong>
      </td>
      </tr>
      <tr>
      <td>MMLU (5-shot)
      </td>
      <td>80.32
      </td>
      <td>80.28
      </td>
      <td>100.0%
      </td>
      </tr>
      <tr>
      <td>ARC Challenge (25-shot)
      </td>
      <td>68.00
      </td>
      <td>67.75
      </td>
      <td>99.6%
      </td>
      </tr>
      <tr>
      <td>GSM-8K (5-shot, strict-match)
      </td>
      <td>89.76
      </td>
      <td>88.55
      </td>
      <td>98.7%
      </td>
      </tr>
      <tr>
      <td>Hellaswag (10-shot)
      </td>
      <td>84.81
      </td>
      <td>84.39
      </td>
      <td>99.5%
      </td>
      </tr>
      <tr>
      <td>Winogrande (5-shot)
      </td>
      <td>81.69
      </td>
      <td>81.77
      </td>
      <td>100.1%
      </td>
      </tr>
      <tr>
      <td>TruthfulQA (0-shot, mc2)
      </td>
      <td>64.93
      </td>
      <td>64.00
      </td>
      <td>98.6%
      </td>
      </tr>
      <tr>
      <td><strong>Average</strong>
      </td>
      <td><strong>78.26</strong>
      </td>
      <td><strong>77.79</strong>
      </td>
      <td><strong>99.4%</strong>
      </td>
      </tr>
    </table>   
  logo: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAACYktHRAAAqo0jMgAAAAlwSFlzAAAAVQAAAFUABu0WZwAAAAd0SU1FB+gMExEDHn1ZgdgAAAABb3JOVAHPoneaAAACbUlEQVRIx93UXUiedRgG8N98nRBN5zrwIDZLpXStT0XWu8JkYM1RLGgfUZQydhA7E+rEYmw5txMN7CDCMSPYYYyCwLCTGQQ528FUNmhK4mt6YH6kNuZcvTvYv2fvl+ZpXSfPdd//+76em+u5/w//W1T73k3j2pw27qY+z228ucg+i06q0mDYsAZVTvnDPkX/3lzgjNsWfBjiTp2B9Vm1ol1BekN+hsCXKtW6igfU6w3ZNzSr9qQH9Sj1ztrvf8ayHYE/blEsTHBCm+1gh2VPrT1B3KBE4GN+0yrP3/g4qki4LG54rQmaDKRETxt3O/LgH9RGM2ZhtwlfZTh0IUsgA3kRe8S3OhwEMaOKccev/kyrbzVnzGu5pM7riXiD2SCdL4ZH1ahR410zXvCWxfsbsSl6/u5T10CF97XqjuSKTZgBWwxpEDMnHmojlEi64mejkoY1pp19oD+wUgu6XDSStT8K/BXcPWbalpSTzSYciKJnfe6sklwe9GsJ7Ib6wMp1+cYvKVavg1cs2AVe8hDYasp5PaZsTal703fOeTiXRKsFpWmS94y67uUoV2/OcRcM5hIoc0txSlxhSaP9lpSjyoqkpG4USaZNFfCqocBiKuWj2aRJTSBuWrkmU17X4XquCV40Fr72T+bS7xzi4Zq1GNFrZy6BbW6pQItLNmedxqN7mobUDzTvC5+IKTNgNeSetzvU1SlUvb4AHyn1te2SUeaQw6DTMRddUre+wKy9EvbncPioI476LPt3lrlj847b622PZeQT6hSrze1DNtpNe88T9vhBB9hjUlKfwo0JcMiPliSci3Zz08ab/2O4Cy0mnnC8jSUfAAAAAElFTkSuQmCC
  language: ["en", "fr", "de", "es", "it", "pt", "zh", "ja", "ru", "ko"]
  license: apache-2.0
  licenseLink: https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md
  maturity: Generally Availaible
  libraryName: transformers
  baseModel:
    - repository: mistralai
    - name: Mistral-Small-24B-Base-2501
  labels:
    - inference
    - transformers
    - neuralmagic
    - redhat
    - llmcompressor
    - quantized
    - fp8
  tasks:
    - text-generation
  createTimeSinceEpoch: 1740960000
  lastUpdateTimeSinceEpoch: 1742428800
  artifacts:
    - protocol: NOT YET IN REGISTRY
      createTimeSinceEpoch: 
      tags: 
      uri: https://huggingface.co/neuralmagic-ent/Mistral-Small-24B-Instruct-2501-FP8-dynamic
- repository: neuralmagic-ent
  name: phi-4-quantized.w4a16
  provider: Neural Magic
  description: This model is designed to accelerate research on language models, for use as a building block for generative AI powered features.
  longDescription: |-
    This model is designed to accelerate research on language models, for use as a building block for generative 
      AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) 
      which require:
        1. Memory/compute constrained environments.
        2. Latency bound scenarios.
        3. Reasoning and logic.
  readme: |-
    # phi-4-quantized.w4a16

    ## Model Overview
    - **Model Architecture:** Phi3ForCausalLM
      - **Input:** Text
      - **Output:** Text
    - **Model Optimizations:**
      - **Weight quantization:** INT4
    - **Intended Use Cases:** This model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:
      1. Memory/compute constrained environments.
      2. Latency bound scenarios.
      3. Reasoning and logic.
    - **Out-of-scope:** This model is not specifically designed or evaluated for all downstream purposes, thus:
      1. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.
      2. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English.
      3. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.
    - **Release Date:** 03/03/2025
    - **Version:** 1.0
    - **Model Developers:** RedHat (Neural Magic)


    ### Model Optimizations

    This model was obtained by quantizing the weights of [phi-4](https://huggingface.co/microsoft/phi-4) to INT4 data type.
    This optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%.

    Only the weights of the linear operators within transformers blocks are quantized.
    Weights are quantized using a symmetric per-group scheme, with group size 128.
    The [GPTQ](https://arxiv.org/abs/2210.17323) algorithm is applied for quantization, as implemented in the [llm-compressor](https://github.com/vllm-project/llm-compressor) library.


    ## Deployment

    This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

    ```python
    from vllm import LLM, SamplingParams
    from transformers import AutoTokenizer
    model_id = "neuralmagic-ent/phi-4-quantized.w4a16"
    number_gpus = 1
    sampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    prompt = "Give me a short introduction to large language model."
    llm = LLM(model=model_id, tensor_parallel_size=number_gpus)
    outputs = llm.generate(prompt, sampling_params)
    generated_text = outputs[0].outputs[0].text
    print(generated_text)
    ```

    vLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

    ## Creation

    <details>
      <summary>Creation details</summary>
      This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. 


      ```python
      from transformers import AutoModelForCausalLM, AutoTokenizer
      from llmcompressor.modifiers.quantization import GPTQModifier
      from llmcompressor.transformers import oneshot
      
      # Load model
      model_stub = "microsoft/phi-4"
      model_name = model_stub.split("/")[-1]
      
      num_samples = 1024
      max_seq_len = 8192
      
      tokenizer = AutoTokenizer.from_pretrained(model_stub)
      
      model = AutoModelForCausalLM.from_pretrained(
          model_stub,
          device_map="auto",
          torch_dtype="auto",
      )
      
      def preprocess_fn(example):
        return {"text": tokenizer.apply_chat_template(example["messages"], add_generation_prompt=False, tokenize=False)}
      
      ds = load_dataset("neuralmagic/LLM_compression_calibration", split="train")
      ds = ds.map(preprocess_fn)
      
      # Configure the quantization algorithm and scheme
      recipe = GPTQModifier(
          targets="Linear",
          scheme="W4A16",
          ignore=["lm_head"],
          dampening_frac=0.01,
      )
      
      # Apply quantization
      oneshot(
          model=model,
          dataset=ds, 
          recipe=recipe,
          max_seq_length=max_seq_len,
          num_calibration_samples=num_samples,
      )
      
      # Save to disk in compressed-tensors format
      save_path = model_name + "-quantized.w4a16
      model.save_pretrained(save_path)
      tokenizer.save_pretrained(save_path)
      print(f"Model and tokenizer saved to: {save_path}")
      ```
    </details>
    


    ## Evaluation

    The model was evaluated on the OpenLLM leaderboard tasks (version 1) with the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) and the [vLLM](https://docs.vllm.ai/en/stable/) engine, using the following command:
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic-ent/phi-4-quantized.w4a16",dtype=auto,gpu_memory_utilization=0.6,max_model_len=4096,enable_chunk_prefill=True,tensor_parallel_size=1 \
      --tasks openllm \
      --batch_size auto
    ```

    ### Accuracy

    #### Open LLM Leaderboard evaluation scores
    <table>
      <tr>
      <td><strong>Benchmark</strong>
      </td>
      <td><strong>phi-4</strong>
      </td>
      <td><strong>phi-4-quantized.w4a16<br>(this model)</strong>
      </td>
      <td><strong>Recovery</strong>
      </td>
      </tr>
      <tr>
      <td>MMLU (5-shot)
      </td>
      <td>80.30
      </td>
      <td>79.87
      </td>
      <td>99.5%
      </td>
      </tr>
      <tr>
      <td>ARC Challenge (25-shot)
      </td>
      <td>64.42
      </td>
      <td>62.88
      </td>
      <td>97.6%
      </td>
      </tr>
      <tr>
      <td>GSM-8K (5-shot, strict-match)
      </td>
      <td>90.07
      </td>
      <td>89.69
      </td>
      <td>99.6%
      </td>
      </tr>
      <tr>
      <td>Hellaswag (10-shot)
      </td>
      <td>84.37
      </td>
      <td>83.42
      </td>
      <td>98.9%
      </td>
      </tr>
      <tr>
      <td>Winogrande (5-shot)
      </td>
      <td>80.58
      </td>
      <td>80.74
      </td>
      <td>100.2%
      </td>
      </tr>
      <tr>
      <td>TruthfulQA (0-shot, mc2)
      </td>
      <td>59.37
      </td>
      <td>59.18
      </td>
      <td>99.7%
      </td>
      </tr>
      <tr>
      <td><strong>Average</strong>
      </td>
      <td><strong>76.52</strong>
      </td>
      <td><strong>75.96</strong>
      </td>
      <td><strong>99.3%</strong>
      </td>
      </tr>
    </table>  
  logo: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAACYktHRAAAqo0jMgAAAAlwSFlzAAAAVQAAAFUABu0WZwAAAAd0SU1FB+gMExEDHn1ZgdgAAAABb3JOVAHPoneaAAACbUlEQVRIx93UXUiedRgG8N98nRBN5zrwIDZLpXStT0XWu8JkYM1RLGgfUZQydhA7E+rEYmw5txMN7CDCMSPYYYyCwLCTGQQ528FUNmhK4mt6YH6kNuZcvTvYv2fvl+ZpXSfPdd//+76em+u5/w//W1T73k3j2pw27qY+z228ucg+i06q0mDYsAZVTvnDPkX/3lzgjNsWfBjiTp2B9Vm1ol1BekN+hsCXKtW6igfU6w3ZNzSr9qQH9Sj1ztrvf8ayHYE/blEsTHBCm+1gh2VPrT1B3KBE4GN+0yrP3/g4qki4LG54rQmaDKRETxt3O/LgH9RGM2ZhtwlfZTh0IUsgA3kRe8S3OhwEMaOKccev/kyrbzVnzGu5pM7riXiD2SCdL4ZH1ahR410zXvCWxfsbsSl6/u5T10CF97XqjuSKTZgBWwxpEDMnHmojlEi64mejkoY1pp19oD+wUgu6XDSStT8K/BXcPWbalpSTzSYciKJnfe6sklwe9GsJ7Ib6wMp1+cYvKVavg1cs2AVe8hDYasp5PaZsTal703fOeTiXRKsFpWmS94y67uUoV2/OcRcM5hIoc0txSlxhSaP9lpSjyoqkpG4USaZNFfCqocBiKuWj2aRJTSBuWrkmU17X4XquCV40Fr72T+bS7xzi4Zq1GNFrZy6BbW6pQItLNmedxqN7mobUDzTvC5+IKTNgNeSetzvU1SlUvb4AHyn1te2SUeaQw6DTMRddUre+wKy9EvbncPioI476LPt3lrlj847b622PZeQT6hSrze1DNtpNe88T9vhBB9hjUlKfwo0JcMiPliSci3Zz08ab/2O4Cy0mnnC8jSUfAAAAAElFTkSuQmCC
  language: ["en"]
  license: mit
  licenseLink: https://huggingface.co/microsoft/phi-4/resolve/main/LICENSE
  maturity: Generally Available
  libraryName: transformers
  baseModel:
    - repository: microsoft
    - name: phi-4
  labels:
    - inference
    - phi
    - nlp
    - math
    - code
    - chat
    - conversational
    - neuralmagic
    - redhat
    - llmcompressor
    - quantized
    - int4
  tasks:
    - text-generation
  createTimeSinceEpoch: 1740960000
  lastUpdateTimeSinceEpoch: 1742428800
  artifacts:
    - protocol: NOT YET IN REGISTRY
      createTimeSinceEpoch: 
      tags: 
      uri: https://huggingface.co/neuralmagic-ent/phi-4-quantized.w4a16/commits/main
- repository: neuralmagic
  name: Mistral-7B-Instruct-v0.3-quantized.w4a16
  provider: Neural Magic
  description: Intended for commercial and research use in English.
  longDescription: |-
    Similarly to Mistral-7B-Instruct-v0.3, this models is intended for assistant-like chat. Quantized 
      version of Mistral-7B-Instruct-v0.3. It achieves an average score of 65.08 on the OpenLLM benchmark 
      (version 1), whereas the unquantized model achieves 66.42.
  readme: |-
    # Mistral-7B-Instruct-v0.3-quantized.w4a16

    ## Model Overview
    - **Model Architecture:** Mistral-v0.3
      - **Input:** Text
      - **Output:** Text
    - **Model Optimizations:**
      - **Weight quantization:** INT4
    - **Intended Use Cases:** Intended for commercial and research use in English. Similarly to [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3), this models is intended for assistant-like chat.
    - **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English.
    - **Release Date:** 7/11/2024
    - **Version:** 1.0
    - **License(s):** [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0)
    - **Model Developers:** Neural Magic

    Quantized version of [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3).
    It achieves an average score of 65.08 on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) benchmark (version 1), whereas the unquantized model achieves 66.42.

    ### Model Optimizations

    This model was obtained by quantizing the weights of [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) to INT4 data type.
    This optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 25%.

    Only the weights of the linear operators within transformers blocks are quantized. Symmetric group-wise quantization is applied, in which a linear scaling per group maps the INT4 and floating point representations of the quantized weights.
    [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) is used for quantization with 1% damping factor, group-size as 128 and 512 sequences sampled from [Open-Platypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus).


    ## Deployment

    ### Use with vLLM

    This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

    ```python
    from vllm import LLM, SamplingParams
    from transformers import AutoTokenizer
    model_id = "neuralmagic/Mistral-7B-Instruct-v0.3-quantized.w4a16"
    sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    messages = [
        {"role": "user", "content": "Who are you? Please reply in pirate speak!"},
    ]
    prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    llm = LLM(model=model_id, tensor_parallel_size=2)
    outputs = llm.generate(prompts, sampling_params)
    generated_text = outputs[0].outputs[0].text
    print(generated_text)
    ```

    vLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

    ### Use with transformers

    This model is supported by Transformers leveraging the integration with the [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) data format.
    The following example contemplates how the model can be used using the `generate()` function.

    ```python
    from transformers import AutoTokenizer, AutoModelForCausalLM
    model_id = "neuralmagic/Mistral-7B-Instruct-v0.3-quantized.w4a16"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype="auto",
        device_map="auto",
    )
    messages = [
        {"role": "user", "content": "Who are you? Please reply in pirate speak!"},
    ]
    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)
    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]
    outputs = model.generate(
        input_ids,
        max_new_tokens=256,
        eos_token_id=terminators,
        do_sample=True,
        temperature=0.6,
        top_p=0.9,
    )
    response = outputs[0][input_ids.shape[-1]:]
    print(tokenizer.decode(response, skip_special_tokens=True))
    ```

    ## Creation

    This model was created by applying the [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) library as presented in the code snipet below.
    Although AutoGPTQ was used for this particular model, Neural Magic is transitioning to using [llm-compressor](https://github.com/vllm-project/llm-compressor) which supports several quantization schemes and models not supported by AutoGPTQ.

    ```python
    from transformers import AutoTokenizer
    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
    from datasets import load_dataset
    import random
    model_id = "mistralai/Mistral-7B-Instruct-v0.3"
    num_samples = 512
    max_seq_len = 4096
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    preprocess_fn = lambda example: {"text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n{text}".format_map(example)}
    dataset_name = "neuralmagic/LLM_compression_calibration"
    dataset = load_dataset(dataset_name, split="train")
    ds = dataset.shuffle().select(range(num_samples))
    ds = ds.map(preprocess_fn)
    examples = [
        tokenizer(
            example["text"], padding=False, max_length=max_seq_len, truncation=True,
        ) for example in ds
    ]
    quantize_config = BaseQuantizeConfig(
      bits=4,
      group_size=128,
      desc_act=True,
      model_file_base_name="model",
      damp_percent=0.1,
    )
    model = AutoGPTQForCausalLM.from_pretrained(
      model_id,
      quantize_config,
      device_map="auto",
    )
    model.quantize(examples)
    model.save_pretrained("Mistral-7B-Instruct-v0.3-quantized.w4a16")
    ```



    ## Evaluation

    The model was evaluated on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) leaderboard tasks (version 1) with the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/383bbd54bc621086e05aa1b030d8d4d5635b25e6) (commit 383bbd54bc621086e05aa1b030d8d4d5635b25e6) and the [vLLM](https://docs.vllm.ai/en/stable/) engine, using the following command:
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/Mistral-7B-Instruct-v0.3-quantized.w4a16",dtype=auto,tensor_parallel_size=2,gpu_memory_utilization=0.4,add_bos_token=True,max_model_len=4096 \
      --tasks openllm \
      --batch_size auto
    ```

    ### Accuracy

    #### Open LLM Leaderboard evaluation scores
    <table>
      <tr>
      <td><strong>Benchmark</strong>
      </td>
      <td><strong>Mistral-7B-Instruct-v0.3 </strong>
      </td>
      <td><strong>Mistral-7B-Instruct-v0.3-quantized.w4a16(this model)</strong>
      </td>
      <td><strong>Recovery</strong>
      </td>
      </tr>
      <tr>
      <td>MMLU (5-shot)
      </td>
      <td>61.84
      </td>
      <td>60.83
      </td>
      <td>98.36%
      </td>
      </tr>
      <tr>
      <td>ARC Challenge (25-shot)
      </td>
      <td>63.73
      </td>
      <td>63.90
      </td>
      <td>100.28%
      </td>
      </tr>
      <tr>
      <td>GSM-8K (5-shot, strict-match)
      </td>
      <td>49.27
      </td>
      <td>43.36
      </td>
      <td>88.01%
      </td>
      </tr>
      <tr>
      <td>Hellaswag (10-shot)
      </td>
      <td>84.84
      </td>
      <td>84.07
      </td>
      <td>99.10%
      </td>
      </tr>
      <tr>
      <td>Winogrande (5-shot)
      </td>
      <td>79.55
      </td>
      <td>79.87
      </td>
      <td>100.40%
      </td>
      </tr>
      <tr>
      <td>TruthfulQA (0-shot)
      </td>
      <td>59.33
      </td>
      <td>58.47
      </td>
      <td>98.56%
      </td>
      </tr>
      <tr>
      <td><strong>Average</strong>
      </td>
      <td><strong>66.42</strong>
      </td>
      <td><strong>65.08</strong>
      </td>
      <td><strong>97.98%</strong>
      </td>
      </tr>
    </table>   
  logo: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAACYktHRAAAqo0jMgAAAAlwSFlzAAAAVQAAAFUABu0WZwAAAAd0SU1FB+gMExEDHn1ZgdgAAAABb3JOVAHPoneaAAACbUlEQVRIx93UXUiedRgG8N98nRBN5zrwIDZLpXStT0XWu8JkYM1RLGgfUZQydhA7E+rEYmw5txMN7CDCMSPYYYyCwLCTGQQ528FUNmhK4mt6YH6kNuZcvTvYv2fvl+ZpXSfPdd//+76em+u5/w//W1T73k3j2pw27qY+z228ucg+i06q0mDYsAZVTvnDPkX/3lzgjNsWfBjiTp2B9Vm1ol1BekN+hsCXKtW6igfU6w3ZNzSr9qQH9Sj1ztrvf8ayHYE/blEsTHBCm+1gh2VPrT1B3KBE4GN+0yrP3/g4qki4LG54rQmaDKRETxt3O/LgH9RGM2ZhtwlfZTh0IUsgA3kRe8S3OhwEMaOKccev/kyrbzVnzGu5pM7riXiD2SCdL4ZH1ahR410zXvCWxfsbsSl6/u5T10CF97XqjuSKTZgBWwxpEDMnHmojlEi64mejkoY1pp19oD+wUgu6XDSStT8K/BXcPWbalpSTzSYciKJnfe6sklwe9GsJ7Ib6wMp1+cYvKVavg1cs2AVe8hDYasp5PaZsTal703fOeTiXRKsFpWmS94y67uUoV2/OcRcM5hIoc0txSlxhSaP9lpSjyoqkpG4USaZNFfCqocBiKuWj2aRJTSBuWrkmU17X4XquCV40Fr72T+bS7xzi4Zq1GNFrZy6BbW6pQItLNmedxqN7mobUDzTvC5+IKTNgNeSetzvU1SlUvb4AHyn1te2SUeaQw6DTMRddUre+wKy9EvbncPioI476LPt3lrlj847b622PZeQT6hSrze1DNtpNe88T9vhBB9hjUlKfwo0JcMiPliSci3Zz08ab/2O4Cy0mnnC8jSUfAAAAAElFTkSuQmCC
  language: ["en"]
  license: apache-2.0
  licenseLink: https://www.apache.org/licenses/LICENSE-2.0
  maturity: Generally Available
  libraryName: transformers
  baseModel:
    - repository: mistralai
    - name: Mistral-7B-Instruct-v0.3
  labels:
    - inference
  tasks:
    - text-generation
  createTimeSinceEpoch: 1720656000
  lastUpdateTimeSinceEpoch: 1741824000
  artifacts:
    - protocol: NOT YET IN REGISTRY
      createTimeSinceEpoch: 
      tags: 
      uri: https://huggingface.co/neuralmagic/Mistral-7B-Instruct-v0.3-quantized.w4a16/commits/main
- repository: neuralmagic
  name: DeepSeek-R1-Distill-Llama-8B-FP8-dynamic
  provider: Neural Magic
  description: Quantized version of DeepSeek-R1-Distill-Llama-8B.
  longDescription: |-
    This model was obtained by quantizing the weights and activations of DeepSeek-R1-Distill-Llama-70B to 
      FP8 data type. This optimization reduces the number of bits per parameter from 16 to 8, reducing the 
      disk size and GPU memory requirements by approximately 50%.

    Only the weights and activations of the linear operators within transformers blocks are quantized. Weights 
      are quantized using a symmetric per-channel scheme, whereas quantizations are quantized using a symmetric 
      per-token scheme. LLM Compressor is used for quantization.
  readme: |-
    # DeepSeek-R1-Distill-Llama-8B-FP8-dynamic

    ## Model Overview
    - **Model Architecture:** LlamaForCausalLM
      - **Input:** Text
      - **Output:** Text
    - **Model Optimizations:**
      - **Weight quantization:** FP8
      - **Activation quantization:** FP8
    - **Release Date:** 2/6/2025
    - **Version:** 1.0
    - **Model Developers:** Neural Magic

    Quantized version of [DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B).

    ### Model Optimizations

    This model was obtained by quantizing the weights and activations of [DeepSeek-R1-Distill-Llama-70B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B) to FP8 data type.
    This optimization reduces the number of bits per parameter from 16 to 8, reducing the disk size and GPU memory requirements by approximately 50%.

    Only the weights and activations of the linear operators within transformers blocks are quantized.
    Weights are quantized using a symmetric per-channel scheme, whereas quantizations are quantized using a symmetric per-token scheme.
    [LLM Compressor](https://github.com/vllm-project/llm-compressor) is used for quantization.


    ## Use with vLLM

    This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

    ```python
    from transformers import AutoTokenizer
    from vllm import LLM, SamplingParams
    number_gpus = 1
    model_name = "neuralmagic/DeepSeek-R1-Distill-Llama-8B-FP8-dynamic"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    sampling_params = SamplingParams(temperature=0.6, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])
    llm = LLM(model=model_name, tensor_parallel_size=number_gpus, trust_remote_code=True)
    messages_list = [
        [{"role": "user", "content": "Who are you? Please respond in pirate speak!"}],
    ]
    prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]
    outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)
    generated_text = [output.outputs[0].text for output in outputs]
    print(generated_text)
    ```

    vLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

    ## Creation

    This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. 


    ```python
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from llmcompressor.modifiers.quantization import QuantizationModifier
    from llmcompressor.transformers import oneshot
    import os
    # Load model
    model_stub = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
    model_name = model_stub.split("/")[-1]
    model = AutoModelForCausalLM.from_pretrained(
        model_stub,
        torch_dtype="auto",
    )
    tokenizer = AutoTokenizer.from_pretrained(model_stub)
    # Configure the quantization algorithm and scheme
    recipe = QuantizationModifier(
        targets="Linear",
        scheme="FP8_DYNAMIC",
        ignore=["lm_head"],
    )
    # Apply quantization
    oneshot(
        model=model,
        recipe=recipe,
    )
    # Save to disk in compressed-tensors format
    save_path = model_name + "-FP8-dynamic
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    print(f"Model and tokenizer saved to: {save_path}")
    ```

    ## Evaluation

    The model was evaluated on OpenLLM Leaderboard [V1](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard) and [V2](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/), using the following commands:

    OpenLLM Leaderboard V1:
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/DeepSeek-R1-Distill-Llama-8B-FP8-dynamic",dtype=auto,max_model_len=4096,enable_chunked_prefill=True \
      --tasks openllm \
      --write_out \
      --batch_size auto \
      --output_path output_dir \
      --show_config
    ```

    OpenLLM Leaderboard V2:
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic/DeepSeek-R1-Distill-Llama-8B-FP8-dynamic",dtype=auto,max_model_len=4096,enable_chunked_prefill=True \
      --apply_chat_template \
      --fewshot_as_multiturn \
      --tasks leaderboard \
      --write_out \
      --batch_size auto \
      --output_path output_dir \
      --show_config
    ```

    ### Accuracy

    <table>
      <thead>
        <tr>
          <th>Category</th>
          <th>Metric</th>
          <th>deepseek-ai/DeepSeek-R1-Distill-Llama-8B</th>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-FP8-dynamic</th>
          <th>Recovery</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td rowspan="4"><b>Reasoning</b></td>
          <td>AIME 2024 (pass@1)</td>
          <td>49.25</td>
          <td>50.83</td>
          <td>103.21%</td>
        </tr>
        <tr>
          <td>MATH-500 (pass@1)</td>
          <td>90.18</td>
          <td>90.24</td>
          <td>100.07%</td>
        </tr>
        <tr>
          <td>GPQA Diamond (pass@1)</td>
          <td>49.27</td>
          <td>48.71</td>
          <td>98.86%</td>
        </tr>
        <tr>
          <td><b>Average Score</b></td>
          <td><b>62.9</b></td>
          <td><b>63.26</b></td>
          <td><b>100.57%</b></td>
        </tr>
        <tr>
          <td rowspan="7"><b>OpenLLM V1</b></td>
          <td>ARC-Challenge (Acc-Norm, 25-shot)</td>
          <td>45.05</td>
          <td>44.88</td>
          <td>99.6%</td>
        </tr>
        <tr>
          <td>GSM8K (Strict-Match, 5-shot)</td>
          <td>62.77</td>
          <td>61.49</td>
          <td>98.0%</td>
        </tr>
        <tr>
          <td>HellaSwag (Acc-Norm, 10-shot)</td>
          <td>76.78</td>
          <td>76.68</td>
          <td>99.9%</td>
        </tr>
        <tr>
          <td>MMLU (Acc, 5-shot)</td>
          <td>55.65</td>
          <td>55.82</td>
          <td>100.3%</td>
        </tr>
        <tr>
          <td>TruthfulQA (MC2, 0-shot)</td>
          <td>50.55</td>
          <td>49.92</td>
          <td>98.8%</td>
        </tr>
        <tr>
          <td>Winogrande (Acc, 5-shot)</td>
          <td>68.51</td>
          <td>67.72</td>
          <td>98.8%</td>
        </tr>
        <tr>
          <td><b>Average Score</b></td>
          <td><b>58.88</b></td>
          <td><b>59.42</b></td>
          <td><b>99.2</b></td>
        </tr>
        <tr>
          <td rowspan="7"><b>OpenLLM V2</b></td>
          <td>IFEval (Inst Level Strict Acc, 0-shot)</td>
          <td>38.37</td>
          <td>38.67</td>
          <td>100.8%</td>
        </tr>
        <tr>
          <td>BBH (Acc-Norm, 3-shot)</td>
          <td>7.43</td>
          <td>7.48</td>
          <td>---</td>
        </tr>
        <tr>
          <td>Math-Hard (Exact-Match, 4-shot)</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>---</td>
        </tr>
        <tr>
          <td>GPQA (Acc-Norm, 0-shot)</td>
          <td>1.51</td>
          <td>0.94</td>
          <td>---</td>
        </tr>
        <tr>
          <td>MUSR (Acc-Norm, 0-shot)</td>
          <td>1.86</td>
          <td>1.27</td>
          <td>---</td>
        </tr>
        <tr>
          <td>MMLU-Pro (Acc, 5-shot)</td>
          <td>1.61</td>
          <td>1.60</td>
          <td>---</td>
        </tr>
        <tr>
          <td><b>Average Score</b></td>
          <td><b>8.47</b></td>
          <td><b>8.33</b></td>
          <td><b>---</b></td>
        </tr>
        <tr>
          <td rowspan="4"><b>Coding</b></td>
          <td>HumanEval (pass@1)</td>
          <td>49.90</td>
          <td>51.20</td>
          <td><b>102.6%</b></td>
        </tr>
        <tr>
          <td>HumanEval (pass@10)</td>
          <td>68.90</td>
          <td>68.20</td>
          <td>99.0%</td>
        </tr>
        <tr>
          <td>HumanEval+ (pass@10)</td>
          <td>44.10</td>
          <td>46.60</td>
          <td>105.7%</td>
        </tr>
        <tr>
          <td>HumanEval+ (pass@10)</td>
          <td>62.90</td>
          <td>62.70</td>
          <td>99.7%</td>
        </tr>
      </tbody>
    </table>
    ## Inference Performance


    This model achieves up to 1.4x speedup in single-stream deployment and up to 1.3x speedup in multi-stream asynchronous deployment, depending on hardware and use-case scenario.
    The following performance benchmarks were conducted with [vLLM](https://docs.vllm.ai/en/latest/) version 0.7.2, and [GuideLLM](https://github.com/neuralmagic/guidellm).

    <details>
    <summary>Benchmarking Command</summary>

    ```
    guidellm --model neuralmagic/DeepSeek-R1-Distill-Llama-8B-FP8-dynamic --target "http://localhost:8000/v1" --data-type emulated --data "prompt_tokens=<prompt_tokens>,generated_tokens=<generated_tokens>" --max seconds 360 --backend aiohttp_server
    ```
    </details>

    ### Single-stream performance (measured with vLLM version 0.7.2)
    <table>
      <thead>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th style="text-align: center;" colspan="2" >Instruction Following<br>256 / 128</th>
          <th style="text-align: center;" colspan="2" >Multi-turn Chat<br>512 / 256</th>
          <th style="text-align: center;" colspan="2" >Docstring Generation<br>768 / 128</th>
          <th style="text-align: center;" colspan="2" >RAG<br>1024 / 128</th>
          <th style="text-align: center;" colspan="2" >Code Completion<br>256 / 1024</th>
          <th style="text-align: center;" colspan="2" >Code Fixing<br>1024 / 1024</th>
          <th style="text-align: center;" colspan="2" >Large Summarization<br>4096 / 512</th>
          <th style="text-align: center;" colspan="2" >Large RAG<br>10240 / 1536</th>
        </tr>
        <tr>
          <th>Hardware</th>
          <th>Model</th>
          <th>Average cost reduction</th>
          <th>Latency (s)</th>
          <th>QPD</th>
          <th>Latency (s)</th>
          <th>QPD</th>
          <th>Latency (s)</th>
          <th>QPD</th>
          <th>Latency (s)</th>
          <th>QPD</th>
          <th>Latency (s)</th>
          <th>QPD</th>
          <th>Latency (s)</th>
          <th>QPD</th>
          <th>Latency (s)</th>
          <th>QPD</th>
          <th>Latency (s)</th>
          <th>QPD</th>
        </tr>
      </thead>
      <tbody style="text-align: center" >
        <tr>
          <th rowspan="3" valign="top">A6000x1</th>
          <th>deepseek-ai/DeepSeek-R1-Distill-Llama-8B</th>
          <td>---</td>
          <td>3.0</td>
          <td>1511</td>
          <td>6.0</td>
          <td>755</td>
          <td>3.0</td>
          <td>1483</td>
          <td>3.1</td>
          <td>1462</td>
          <td>23.6</td>
          <td>191</td>
          <td>24.0</td>
          <td>188</td>
          <td>12.7</td>
          <td>353</td>
          <td>41.1</td>
          <td>110</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-quantized.w8a8</th>
          <td>1.53</td>
          <td>1.9</td>
          <td>2356</td>
          <td>3.8</td>
          <td>1175</td>
          <td>2.0</td>
          <td>2291</td>
          <td>2.0</td>
          <td>2207</td>
          <td>15.2</td>
          <td>297</td>
          <td>15.5</td>
          <td>290</td>
          <td>8.5</td>
          <td>531</td>
          <td>28.6</td>
          <td>157</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-quantized.w4a16</th>
          <td>2.35</td>
          <td>1.2</td>
          <td>3870</td>
          <td>2.3</td>
          <td>1918</td>
          <td>1.3</td>
          <td>3492</td>
          <td>1.3</td>
          <td>3335</td>
          <td>9.1</td>
          <td>492</td>
          <td>9.5</td>
          <td>472</td>
          <td>5.8</td>
          <td>771</td>
          <td>22.7</td>
          <td>198</td>
        </tr>
        <tr>
          <th rowspan="3" valign="top">A100x1</th>
          <th>deepseek-ai/DeepSeek-R1-Distill-Llama-8B</th>
          <td>---</td>
          <td>1.5</td>
          <td>1308</td>
          <td>3.1</td>
          <td>657</td>
          <td>1.6</td>
          <td>1274</td>
          <td>1.6</td>
          <td>1263</td>
          <td>12.1</td>
          <td>166</td>
          <td>12.4</td>
          <td>162</td>
          <td>6.5</td>
          <td>308</td>
          <td>25.6</td>
          <td>78</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-quantized.w8a8</th>
          <td>1.30</td>
          <td>1.1</td>
          <td>1763</td>
          <td>2.3</td>
          <td>882</td>
          <td>1.2</td>
          <td>1716</td>
          <td>1.2</td>
          <td>1698</td>
          <td>9.0</td>
          <td>223</td>
          <td>9.2</td>
          <td>218</td>
          <td>4.9</td>
          <td>409</td>
          <td>25.7</td>
          <td>78</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-quantized.w4a16</th>
          <td>1.76</td>
          <td>0.8</td>
          <td>2501</td>
          <td>1.6</td>
          <td>1236</td>
          <td>0.9</td>
          <td>2350</td>
          <td>0.9</td>
          <td>2287</td>
          <td>6.4</td>
          <td>316</td>
          <td>6.6</td>
          <td>306</td>
          <td>3.7</td>
          <td>544</td>
          <td>24.7</td>
          <td>82</td>
        </tr>
        <tr>
          <th rowspan="3" valign="top">H100x1</th>
          <th>deepseek-ai/DeepSeek-R1-Distill-Llama-8B</th>
          <td>---</td>
          <td>1.0</td>
          <td>1146</td>
          <td>1.9</td>
          <td>574</td>
          <td>1.0</td>
          <td>1128</td>
          <td>1.0</td>
          <td>1111</td>
          <td>7.6</td>
          <td>144</td>
          <td>7.7</td>
          <td>142</td>
          <td>4.1</td>
          <td>266</td>
          <td>16.3</td>
          <td>67</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-FP8-dynamic</th>
          <td>1.25</td>
          <td>0.7</td>
          <td>1567</td>
          <td>1.4</td>
          <td>758</td>
          <td>0.7</td>
          <td>1484</td>
          <td>0.7</td>
          <td>1462</td>
          <td>5.7</td>
          <td>191</td>
          <td>5.8</td>
          <td>189</td>
          <td>3.2</td>
          <td>347</td>
          <td>22.5</td>
          <td>49</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-quantized.w4a16</th>
          <td>1.30</td>
          <td>0.7</td>
          <td>1527</td>
          <td>1.4</td>
          <td>768</td>
          <td>0.7</td>
          <td>1495</td>
          <td>0.7</td>
          <td>1463</td>
          <td>5.6</td>
          <td>194</td>
          <td>5.7</td>
          <td>190</td>
          <td>3.1</td>
          <td>350</td>
          <td>14.7</td>
          <td>74</td>
        </tr>
      </tbody>
    </table>
    **Use case profiles: prompt tokens / generation tokens
    **QPD: Queries per dollar, based on on-demand cost at [Lambda Labs](https://lambdalabs.com/service/gpu-cloud) (observed on 2/18/2025).


    ### Multi-stream asynchronous performance (measured with vLLM version 0.7.2)
    <table>
      <thead>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th style="text-align: center;" colspan="2" >Instruction Following<br>256 / 128</th>
          <th style="text-align: center;" colspan="2" >Multi-turn Chat<br>512 / 256</th>
          <th style="text-align: center;" colspan="2" >Docstring Generation<br>768 / 128</th>
          <th style="text-align: center;" colspan="2" >RAG<br>1024 / 128</th>
          <th style="text-align: center;" colspan="2" >Code Completion<br>256 / 1024</th>
          <th style="text-align: center;" colspan="2" >Code Fixing<br>1024 / 1024</th>
          <th style="text-align: center;" colspan="2" >Large Summarization<br>4096 / 512</th>
          <th style="text-align: center;" colspan="2" >Large RAG<br>10240 / 1536</th>
        </tr>
        <tr>
          <th>Hardware</th>
          <th>Model</th>
          <th>Average cost reduction</th>
          <th>Maximum throughput (QPS)</th>
          <th>QPD</th>
          <th>Maximum throughput (QPS)</th>
          <th>QPD</th>
          <th>Maximum throughput (QPS)</th>
          <th>QPD</th>
          <th>Maximum throughput (QPS)</th>
          <th>QPD</th>
          <th>Maximum throughput (QPS)</th>
          <th>QPD</th>
          <th>Maximum throughput (QPS)</th>
          <th>QPD</th>
          <th>Maximum throughput (QPS)</th>
          <th>QPD</th>
          <th>Maximum throughput (QPS)</th>
          <th>QPD</th>
        </tr>
      </thead>
      <tbody style="text-align: center" >
        <tr>
          <th rowspan="3" valign="top">A6000x1</th>
          <th>deepseek-ai/DeepSeek-R1-Distill-Llama-8B</th>
          <td>---</td>
          <td>12.6</td>
          <td>56742</td>
          <td>5.7</td>
          <td>25687</td>
          <td>6.5</td>
          <td>29349</td>
          <td>5.2</td>
          <td>23259</td>
          <td>1.6</td>
          <td>7250</td>
          <td>1.2</td>
          <td>5181</td>
          <td>0.8</td>
          <td>3445</td>
          <td>0.1</td>
          <td>616</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-quantized.w8a8</th>
          <td>1.34</td>
          <td>17.4</td>
          <td>78101</td>
          <td>7.6</td>
          <td>34351</td>
          <td>8.8</td>
          <td>39790</td>
          <td>7.0</td>
          <td>31532</td>
          <td>2.3</td>
          <td>10405</td>
          <td>1.5</td>
          <td>6960</td>
          <td>1.0</td>
          <td>4355</td>
          <td>0.2</td>
          <td>785</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-quantized.w4a16</th>
          <td>0.91</td>
          <td>10.9</td>
          <td>48964</td>
          <td>5.1</td>
          <td>22989</td>
          <td>4.8</td>
          <td>21791</td>
          <td>3.8</td>
          <td>17039</td>
          <td>2.2</td>
          <td>9726</td>
          <td>1.2</td>
          <td>5434</td>
          <td>0.6</td>
          <td>2544</td>
          <td>0.1</td>
          <td>578</td>
        </tr>
        <tr>
          <th rowspan="3" valign="top">A100x1</th>
          <th>deepseek-ai/DeepSeek-R1-Distill-Llama-8B</th>
          <td>---</td>
          <td>24.5</td>
          <td>49296</td>
          <td>11.3</td>
          <td>22657</td>
          <td>13.0</td>
          <td>26047</td>
          <td>10.5</td>
          <td>21020</td>
          <td>3.5</td>
          <td>7029</td>
          <td>2.5</td>
          <td>4995</td>
          <td>1.7</td>
          <td>3503</td>
          <td>0.3</td>
          <td>659</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-quantized.w8a8</th>
          <td>1.27</td>
          <td>30.8</td>
          <td>62042</td>
          <td>14.1</td>
          <td>28419</td>
          <td>17.2</td>
          <td>34554</td>
          <td>13.8</td>
          <td>27719</td>
          <td>4.6</td>
          <td>9299</td>
          <td>3.1</td>
          <td>6215</td>
          <td>2.2</td>
          <td>4331</td>
          <td>0.4</td>
          <td>807</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-quantized.w4a16</th>
          <td>0.97</td>
          <td>22.7</td>
          <td>45708</td>
          <td>10.5</td>
          <td>21216</td>
          <td>11.1</td>
          <td>22353</td>
          <td>8.9</td>
          <td>17939</td>
          <td>3.9</td>
          <td>7758</td>
          <td>2.6</td>
          <td>5241</td>
          <td>1.6</td>
          <td>3196</td>
          <td>0.4</td>
          <td>718</td>
        </tr>
        <tr>
          <th rowspan="3" valign="top">H100x1</th>
          <th>deepseek-ai/DeepSeek-R1-Distill-Llama-8B</th>
          <td>---</td>
          <td>49.0</td>
          <td>53593</td>
          <td>22.6</td>
          <td>24750</td>
          <td>28.3</td>
          <td>30971</td>
          <td>22.9</td>
          <td>25035</td>
          <td>7.2</td>
          <td>7912</td>
          <td>5.1</td>
          <td>5561</td>
          <td>3.6</td>
          <td>3939</td>
          <td>0.6</td>
          <td>703</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-FP8-dynamic</th>
          <td>1.14</td>
          <td>57.1</td>
          <td>62517</td>
          <td>26.0</td>
          <td>28440</td>
          <td>34.5</td>
          <td>37781</td>
          <td>28.7</td>
          <td>31360</td>
          <td>7.2</td>
          <td>7877</td>
          <td>5.4</td>
          <td>5923</td>
          <td>4.3</td>
          <td>4697</td>
          <td>0.7</td>
          <td>782</td>
        </tr>
        <tr>
          <th>neuralmagic/DeepSeek-R1-Distill-Llama-8B-quantized.w4a16</th>
          <td>1.01</td>
          <td>49.8</td>
          <td>54452</td>
          <td>22.9</td>
          <td>25035</td>
          <td>28.5</td>
          <td>31162</td>
          <td>23.0</td>
          <td>25200</td>
          <td>6.8</td>
          <td>7493</td>
          <td>5.0</td>
          <td>5431</td>
          <td>3.7</td>
          <td>4079</td>
          <td>0.7</td>
          <td>787</td>
        </tr>
      </tbody>
    </table>
    **Use case profiles: prompt tokens / generation tokens
    **QPS: Queries per second.

    **QPD: Queries per dollar, based on on-demand cost at [Lambda Labs](https://lambdalabs.com/service/gpu-cloud) (observed on 2/18/2025).   
  logo: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAACYktHRAAAqo0jMgAAAAlwSFlzAAAAVQAAAFUABu0WZwAAAAd0SU1FB+gMExEDHn1ZgdgAAAABb3JOVAHPoneaAAACbUlEQVRIx93UXUiedRgG8N98nRBN5zrwIDZLpXStT0XWu8JkYM1RLGgfUZQydhA7E+rEYmw5txMN7CDCMSPYYYyCwLCTGQQ528FUNmhK4mt6YH6kNuZcvTvYv2fvl+ZpXSfPdd//+76em+u5/w//W1T73k3j2pw27qY+z228ucg+i06q0mDYsAZVTvnDPkX/3lzgjNsWfBjiTp2B9Vm1ol1BekN+hsCXKtW6igfU6w3ZNzSr9qQH9Sj1ztrvf8ayHYE/blEsTHBCm+1gh2VPrT1B3KBE4GN+0yrP3/g4qki4LG54rQmaDKRETxt3O/LgH9RGM2ZhtwlfZTh0IUsgA3kRe8S3OhwEMaOKccev/kyrbzVnzGu5pM7riXiD2SCdL4ZH1ahR410zXvCWxfsbsSl6/u5T10CF97XqjuSKTZgBWwxpEDMnHmojlEi64mejkoY1pp19oD+wUgu6XDSStT8K/BXcPWbalpSTzSYciKJnfe6sklwe9GsJ7Ib6wMp1+cYvKVavg1cs2AVe8hDYasp5PaZsTal703fOeTiXRKsFpWmS94y67uUoV2/OcRcM5hIoc0txSlxhSaP9lpSjyoqkpG4USaZNFfCqocBiKuWj2aRJTSBuWrkmU17X4XquCV40Fr72T+bS7xzi4Zq1GNFrZy6BbW6pQItLNmedxqN7mobUDzTvC5+IKTNgNeSetzvU1SlUvb4AHyn1te2SUeaQw6DTMRddUre+wKy9EvbncPioI476LPt3lrlj847b622PZeQT6hSrze1DNtpNe88T9vhBB9hjUlKfwo0JcMiPliSci3Zz08ab/2O4Cy0mnnC8jSUfAAAAAElFTkSuQmCC
  language: ["en"]
  license: mit
  licenseLink: https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md
  maturity: Generally Available
  libraryName: transformers
  baseModel:
    - repository: deepseek-ai
    - name: DeepSeek-R1-Distill-Llama-8B
  labels:
    - inference
    - deepseek
    - fp8
    - vllm
  tasks:
    - text-generation
  createTimeSinceEpoch: 1738368000
  lastUpdateTimeSinceEpoch: 1740528000
  artifacts:
    - protocol: NOT YET IN REGISTRY
      createTimeSinceEpoch: 
      tags: 
      uri: https://huggingface.co/neuralmagic/DeepSeek-R1-Distill-Llama-8B-FP8-dynamic/commits/main
- repository: neuralmagic
  name: Qwen2.5-VL-72B-Instruct-quantized.w8a8
  provider: Neural Magic
  description: Quantized version of Qwen/Qwen2.5-VL-72B-Instruct.
  longDescription: |-
    This model was obtained by quantizing the weights of Qwen/Qwen2.5-VL-72B-Instruct to INT8 data type,  
      ready for inference with vLLM >= 0.5.2.
  readme: |-
    # Qwen2.5-VL-72B-Instruct-quantized-w8a8

    ## Model Overview
    - **Model Architecture:** Qwen/Qwen2.5-VL-72B-Instruct
      - **Input:** Vision-Text
      - **Output:** Text
    - **Model Optimizations:**
      - **Weight quantization:** INT8
      - **Activation quantization:** INT8
    - **Release Date:** 2/24/2025
    - **Version:** 1.0
    - **Model Developers:** Neural Magic

    Quantized version of [Qwen/Qwen2.5-VL-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct).

    ### Model Optimizations

    This model was obtained by quantizing the weights of [Qwen/Qwen2.5-VL-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct) to INT8 data type, ready for inference with vLLM >= 0.5.2.

    ## Deployment

    ### Use with vLLM

    This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

    ```python
    from vllm.assets.image import ImageAsset
    from vllm import LLM, SamplingParams
    # prepare model
    llm = LLM(
        model="neuralmagic/Qwen2.5-VL-72B-Instruct-quantized.w8a8",
        trust_remote_code=True,
        max_model_len=4096,
        max_num_seqs=2,
    )
    # prepare inputs
    question = "What is the content of this image?"
    inputs = {
        "prompt": f"<|user|>\n<|image_1|>\n{question}<|end|>\n<|assistant|>\n",
        "multi_modal_data": {
            "image": ImageAsset("cherry_blossom").pil_image.convert("RGB")
        },
    }
    # generate response
    print("========== SAMPLE GENERATION ==============")
    outputs = llm.generate(inputs, SamplingParams(temperature=0.2, max_tokens=64))
    print(f"PROMPT  : {outputs[0].prompt}")
    print(f"RESPONSE: {outputs[0].outputs[0].text}")
    print("==========================================")
    ```

    vLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

    ## Creation

    This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below as part a multimodal announcement blog.

    <details>
      <summary>Model Creation Code</summary>
      
    ```python
    import base64
    from io import BytesIO
    import torch
    from datasets import load_dataset
    from qwen_vl_utils import process_vision_info
    from transformers import AutoProcessor
    from llmcompressor.modifiers.quantization import GPTQModifier
    from llmcompressor.transformers import oneshot
    from llmcompressor.transformers.tracing import (
        TraceableQwen2_5_VLForConditionalGeneration,
    )
    # Load model.
    model_id = "Qwen/Qwen2.5-VL-72B-Instruct"
    model = TraceableQwen2_5_VLForConditionalGeneration.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype="auto",
    )
    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
    # Oneshot arguments
    DATASET_ID = "lmms-lab/flickr30k"
    DATASET_SPLIT = {"calibration": "test[:512]"}
    NUM_CALIBRATION_SAMPLES = 512
    MAX_SEQUENCE_LENGTH = 2048
    # Load dataset and preprocess.
    ds = load_dataset(DATASET_ID, split=DATASET_SPLIT)
    ds = ds.shuffle(seed=42)
    dampening_frac=0.01
    # Apply chat template and tokenize inputs.
    def preprocess_and_tokenize(example):
        # preprocess
        buffered = BytesIO()
        example["image"].save(buffered, format="PNG")
        encoded_image = base64.b64encode(buffered.getvalue())
        encoded_image_text = encoded_image.decode("utf-8")
        base64_qwen = f"data:image;base64,{encoded_image_text}"
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": base64_qwen},
                    {"type": "text", "text": "What does the image show?"},
                ],
            }
        ]
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        # tokenize
        return processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=False,
            max_length=MAX_SEQUENCE_LENGTH,
            truncation=True,
        )
    ds = ds.map(preprocess_and_tokenize, remove_columns=ds["calibration"].column_names)
    # Define a oneshot data collator for multimodal inputs.
    def data_collator(batch):
        assert len(batch) == 1
        return {key: torch.tensor(value) for key, value in batch[0].items()}
    # Recipe
    recipe = [
        GPTQModifier(
            targets="Linear",
            scheme="W8A8",
            sequential_targets=["Qwen2_5_VLDecoderLayer"],
            ignore=["lm_head", "re:visual.*"],
        ),
    ]
    SAVE_DIR==f"{model_id.split('/')[1]}-quantized.w8a8"
    # Perform oneshot
    oneshot(
        model=model,
        tokenizer=model_id,
        dataset=ds,
        recipe=recipe,
        max_seq_length=MAX_SEQUENCE_LENGTH,
        num_calibration_samples=NUM_CALIBRATION_SAMPLES,
        trust_remote_code_model=True,
        data_collator=data_collator,
        output_dir=SAVE_DIR
    )
    ```
    </details>

    ## Evaluation

    The model was evaluated using [mistral-evals](https://github.com/neuralmagic/mistral-evals) for vision-related tasks and using [lm_evaluation_harness](https://github.com/neuralmagic/lm-evaluation-harness) for select text-based benchmarks. The evaluations were conducted using the following commands:

    <details>
    <summary>Evaluation Commands</summary>
      
    ### Vision Tasks
    - vqav2
    - docvqa
    - mathvista
    - mmmu
    - chartqa

    ```
    vllm serve neuralmagic/pixtral-12b-quantized.w8a8 --tensor_parallel_size 1 --max_model_len 25000 --trust_remote_code --max_num_seqs 8 --gpu_memory_utilization 0.9 --dtype float16 --limit_mm_per_prompt image=7
    python -m eval.run eval_vllm \
            --model_name neuralmagic/pixtral-12b-quantized.w8a8 \
            --url http://0.0.0.0:8000 \
            --output_dir ~/tmp \
            --eval_name <vision_task_name>
    ```

    ### Text-based Tasks
    #### MMLU
      
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="<model_name>",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=<n>,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \
      --tasks mmlu \
      --num_fewshot 5 \
      --batch_size auto \
      --output_path output_dir
    ```

    #### MGSM

    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="<model_name>",dtype=auto,max_model_len=4096,max_gen_toks=2048,max_num_seqs=128,tensor_parallel_size=<n>,gpu_memory_utilization=0.9 \
      --tasks mgsm_cot_native \
      --num_fewshot 0 \
      --batch_size auto \
      --output_path output_dir
    ```
    </details>


    ### Accuracy

    <table>
      <thead>
        <tr>
          <th>Category</th>
          <th>Metric</th>
          <th>Qwen/Qwen2.5-VL-72B-Instruct</th>
          <th>neuralmagic/Qwen2.5-VL-72B-Instruct-quantized.w8a8</th>
          <th>Recovery (%)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td rowspan="6"><b>Vision</b></td>
          <td>MMMU (val, CoT)<br><i>explicit_prompt_relaxed_correctness</i></td>
          <td>64.33</td>
          <td>67.56</td>
          <td>105.02%</td>
        </tr>
        <tr>
          <td>VQAv2 (val)<br><i>vqa_match</i></td>
          <td>81.94</td>
          <td>81.91</td>
          <td>99.96%</td>
        </tr>
        <tr>
          <td>DocVQA (val)<br><i>anls</i></td>
          <td>94.71</td>
          <td>94.71</td>
          <td>100.00%</td>
        </tr>
        <tr>
          <td>ChartQA (test, CoT)<br><i>anywhere_in_answer_relaxed_correctness</i></td>
          <td>88.96</td>
          <td>89.40</td>
          <td>100.49%</td>
        </tr>
        <tr>
          <td>Mathvista (testmini, CoT)<br><i>explicit_prompt_relaxed_correctness</i></td>
          <td>78.18</td>
          <td>78.38</td>
          <td>100.26%</td>
        </tr>
        <tr>
          <td><b>Average Score</b></td>
          <td><b>81.62</b></td>
          <td><b>82.00</b></td>
          <td><b>100.46%</b></td>
        </tr>
        <tr>
          <td rowspan="2"><b>Text</b></td>
          <td>MGSM (CoT)</td>
          <td>75.45</td>
          <td>74.29</td>
          <td>98.46%</td>
        </tr>
        <tr>
          <td>MMLU (5-shot)</td>
          <td>86.16</td>
          <td>85.65</td>
          <td>99.41%</td>
        </tr>
      </tbody>
    </table>

    ## Inference Performance


    This model achieves up to 1.87x speedup in single-stream deployment and up to 1.9x speedup in multi-stream asynchronous deployment, depending on hardware and use-case scenario.
    The following performance benchmarks were conducted with [vLLM](https://docs.vllm.ai/en/latest/) version 0.7.2, and [GuideLLM](https://github.com/neuralmagic/guidellm).

    <details>
    <summary>Benchmarking Command</summary>
    ```
      guidellm --model neuralmagic/Qwen2.5-VL-72B-Instruct-quantized.w8a8 --target "http://localhost:8000/v1" --data-type emulated --data prompt_tokens=<prompt_tokens>,generated_tokens=<generated_tokens>,images=<num_images>,width=<image_width>,height=<image_height> --max seconds 120 --backend aiohttp_server
    ```

    </details>

    ### Single-stream performance (measured with vLLM version 0.7.2)

    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th style="text-align: center;" colspan="2" >Document Visual Question Answering<br>1680W x 2240H<br>64/128</th>
          <th style="text-align: center;" colspan="2" >Visual Reasoning <br>640W x 480H<br>128/128</th>
          <th style="text-align: center;" colspan="2" >Image Captioning<br>480W x 360H<br>0/128</th>
        </tr>
        <tr>
          <th>Hardware</th>
          <th>Number of GPUs</th>
          <th>Model</th>
          <th>Average Cost Reduction</th>
          <th>Latency (s)</th>
          <th>Queries Per Dollar</th>
          <th>Latency (s)th>
          <th>Queries Per Dollar</th>
          <th>Latency (s)</th>
          <th>Queries Per Dollar</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th rowspan="3" valign="top">A100</td>
          <td>4</td>
          <td>Qwen/Qwen2.5-VL-72B-Instruct</td>
          <td></td>
          <td>6.4</td>
          <td>78</td>
          <td>4.5</td>
          <td>111</td>
          <td>4.4</td>
          <td>113</td>
        </tr>
        <tr>
          <td>2</td>
          <td>neuralmagic/Qwen2.5-VL-72B-Instruct-quantized.w8a8</td>
          <td>1.85</td>
          <td>7.0</td>
          <td>143</td>
          <td>4.9</td>
          <td>205</td>
          <td>4.8</td>
          <td>211</td>
        </tr>
        <tr>
          <td>1</td>
          <td>neuralmagic/Qwen2.5-VL-72B-Instruct-quantized.w4a16</td>
          <td>3.33</td>
          <td>9.4</td>
          <td>213</td>
          <td>5.1</td>
          <td>396</td>
          <td>4.8</td>
          <td>420</td>
        </tr>
        <tr>
          <th rowspan="3" valign="top">H100</td>
          <td>4</td>
          <td>Qwen/Qwen2.5-VL-72B-Instruct</td>
          <td></td>
          <td>4.3</td>
          <td>68</td>
          <td>3.0</td>
          <td>97</td>
          <td>2.9</td>
          <td>100</td>
        </tr>
        <tr>
          <td>2</td>
          <td>neuralmagic/Qwen2.5-VL-72B-Instruct-FP8-Dynamic</td>
          <td>1.79</td>
          <td>4.6</td>
          <td>122</td>
          <td>3.3</td>
          <td>173</td>
          <td>3.2</td>
          <td>177</td>
        </tr>
        <tr>
          <td>1</td>
          <td>neuralmagic/Qwen2.5-VL-72B-Instruct-quantized.w4a16</td>
          <td>5.66</td>
          <td>4.3</td>
          <td>252</td>
          <td>4.4</td>
          <td>251</td>
          <td>4.2</td>
          <td>259</td>
        </tr>
      </tbody>
    </table>
    **Use case profiles: Image Size (WxH) / prompt tokens / generation tokens
    **QPD: Queries per dollar, based on on-demand cost at [Lambda Labs](https://lambdalabs.com/service/gpu-cloud) (observed on 2/18/2025).

    ### Multi-stream asynchronous performance (measured with vLLM version 0.7.2)

    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th style="text-align: center;" colspan="2" >Document Visual Question Answering<br>1680W x 2240H<br>64/128</th>
          <th style="text-align: center;" colspan="2" >Visual Reasoning <br>640W x 480H<br>128/128</th>
          <th style="text-align: center;" colspan="2" >Image Captioning<br>480W x 360H<br>0/128</th>
        </tr>
        <tr>
          <th>Hardware</th>
          <th>Model</th>
          <th>Average Cost Reduction</th>
          <th>Maximum throughput (QPS)</th>
          <th>Queries Per Dollar</th>
          <th>Maximum throughput (QPS)</th>
          <th>Queries Per Dollar</th>
          <th>Maximum throughput (QPS)</th>
          <th>Queries Per Dollar</th>
        </tr>
      </thead>
      <tbody style="text-align: center">
        <tr>
          <th rowspan="3" valign="top">A100x4</th>
          <td>Qwen/Qwen2.5-VL-72B-Instruct</td>
          <td></td>
          <td>0.4</td>
          <td>180</td>
          <td>1.1</td>
          <td>539</td>
          <td>1.2</td>
          <td>595</td>
        </tr>
        <tr>
          <td>neuralmagic/Qwen2.5-VL-72B-Instruct-quantized.w8a8</td>
          <td>1.80</td>
          <td>0.6</td>
          <td>289</td>
          <td>2.0</td>
          <td>1020</td>
          <td>2.3</td>
          <td>1133</td>
        </tr>
        <tr>
          <td>neuralmagic/Qwen2.5-VL-72B-Instruct-quantized.w4a16</td>
          <td>2.75</td>
          <td>0.7</td>
          <td>341</td>
          <td>3.2</td>
          <td>1588</td>
          <td>4.1</td>
          <td>2037</td>
        </tr>
        <tr>
          <th rowspan="3" valign="top">H100x4</th>
          <td>Qwen/Qwen2.5-VL-72B-Instruct</td>
          <td></td>
          <td>0.5</td>
          <td>134</td>
          <td>1.2</td>
          <td>357</td>
          <td>1.3</td>
          <td>379</td>
        </tr>
        <tr>
          <td>neuralmagic/Qwen2.5-VL-72B-Instruct-FP8-Dynamic</td>
          <td>1.73</td>
          <td>0.9</td>
          <td>247</td>
          <td>2.2</td>
          <td>621</td>
          <td>2.4</td>
          <td>669</td>
        </tr>
        <tr>
          <td>neuralmagic/Qwen2.5-VL-72B-Instruct-quantized.w4a16</td>
          <td>8.27</td>
          <td>3.3</td>
          <td>913</td>
          <td>3.3</td>
          <td>898</td>
          <td>3.6</td>
          <td>991</td>
        </tr>
      </tbody>
    </table>
    **Use case profiles: Image Size (WxH) / prompt tokens / generation tokens
    **QPS: Queries per second.

    **QPD: Queries per dollar, based on on-demand cost at [Lambda Labs](https://lambdalabs.com/service/gpu-cloud) (observed on 2/18/2025).    
  logo: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAACYktHRAAAqo0jMgAAAAlwSFlzAAAAVQAAAFUABu0WZwAAAAd0SU1FB+gMExEDHn1ZgdgAAAABb3JOVAHPoneaAAACbUlEQVRIx93UXUiedRgG8N98nRBN5zrwIDZLpXStT0XWu8JkYM1RLGgfUZQydhA7E+rEYmw5txMN7CDCMSPYYYyCwLCTGQQ528FUNmhK4mt6YH6kNuZcvTvYv2fvl+ZpXSfPdd//+76em+u5/w//W1T73k3j2pw27qY+z228ucg+i06q0mDYsAZVTvnDPkX/3lzgjNsWfBjiTp2B9Vm1ol1BekN+hsCXKtW6igfU6w3ZNzSr9qQH9Sj1ztrvf8ayHYE/blEsTHBCm+1gh2VPrT1B3KBE4GN+0yrP3/g4qki4LG54rQmaDKRETxt3O/LgH9RGM2ZhtwlfZTh0IUsgA3kRe8S3OhwEMaOKccev/kyrbzVnzGu5pM7riXiD2SCdL4ZH1ahR410zXvCWxfsbsSl6/u5T10CF97XqjuSKTZgBWwxpEDMnHmojlEi64mejkoY1pp19oD+wUgu6XDSStT8K/BXcPWbalpSTzSYciKJnfe6sklwe9GsJ7Ib6wMp1+cYvKVavg1cs2AVe8hDYasp5PaZsTal703fOeTiXRKsFpWmS94y67uUoV2/OcRcM5hIoc0txSlxhSaP9lpSjyoqkpG4USaZNFfCqocBiKuWj2aRJTSBuWrkmU17X4XquCV40Fr72T+bS7xzi4Zq1GNFrZy6BbW6pQItLNmedxqN7mobUDzTvC5+IKTNgNeSetzvU1SlUvb4AHyn1te2SUeaQw6DTMRddUre+wKy9EvbncPioI476LPt3lrlj847b622PZeQT6hSrze1DNtpNe88T9vhBB9hjUlKfwo0JcMiPliSci3Zz08ab/2O4Cy0mnnC8jSUfAAAAAElFTkSuQmCC
  language: ["en"]
  license: apache-2.0
  licenseLink: https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md
  maturity: Generally Available
  libraryName: transformers
  baseModel:
    - repository: Qwen
    - name: Qwen2.5-VL-72B-Instruct
  labels:
    - inference
    - vllm
    - vision
    - w8a8
  tasks:
    - image-text-to-text
  createTimeSinceEpoch: 1738886400
  lastUpdateTimeSinceEpoch: 1741132800
  artifacts:
    - protocol: NOT YET IN REGISTRY
      createTimeSinceEpoch: 
      tags: 
      uri: https://huggingface.co/neuralmagic/Qwen2.5-VL-72B-Instruct-quantized.w8a8/commits/main
- repository: neuralmagic-ent
  name: Mixtral-8x22B-v0.1-quantized.w4a16
  provider: Neural Magic
  description: Quantized version of Mixtral-8x22B-v0.1. It achieves an average score of 74.17 on the OpenLLM benchmark (version 1), whereas the unquantized model achieves 74.69.
  longDescription: |-
    This model was obtained by only quantizing the weights to INT4 data type, ready for inference with 
      vLLM >= 0.5.2. This optimization reduces the number of bits per parameter from 16 to 4, reducing 
      the disk size and GPU memory requirements by approximately 75%. Only the weights of the linear 
      operators within transformers blocks are quantized, except the MLP routers.
  readme: |-
    # Mixtral-8x22B-v0.1-quantized.w4a16

    ## Model Overview
    - **Model Architecture:** Mixtral-8x22B-v0.1
      - **Input:** Text
      - **Output:** Text
    - **Model Optimizations:**
      - **Weight quantization:** INT4
      - **Activation quantization:** None
    - **Release Date:** 3/1/2025
    - **Version:** 1.0
    - **Model Developers:** Neural Magic

    Quantized version of [Mixtral-8x22B-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-v0.1).
    It achieves an average score of 74.17 on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) benchmark (version 1), whereas the unquantized model achieves 74.69.

    ### Model Optimizations

    This model was obtained by only quantizing the weights to INT4 data type, ready for inference with vLLM >= 0.5.2.
    This optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%. Only the weights of the linear operators within transformers blocks are quantized, except the MLP routers. 

    ## Deployment

    ### Use with vLLM

    This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

    ```python
    from transformers import AutoTokenizer
    from vllm import LLM, SamplingParams
    max_model_len, tp_size = 4096, 4
    model_name = "neuralmagic-ent/Mixtral-8x22B-v0.1-quantized.w4a16"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    llm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True)
    sampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])
    messages_list = [
        [{"role": "user", "content": "Who are you? Please respond in pirate speak!"}],
    ]
    prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]
    outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)
    generated_text = [output.outputs[0].text for output in outputs]
    print(generated_text)
    ```

    vLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

    ## Creation

    This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below with the following command:

    ```bash
    python quantize.py --model_path mistralai/Mixtral-8x22B-v0.1 --quant_path "output_dir" --calib_size 1024 --dampening_frac 0.1 --observer minmax --actorder False 
    ```


    ```python
    from datasets import load_dataset
    from transformers import AutoTokenizer
    from llmcompressor.modifiers.quantization import GPTQModifier
    from llmcompressor.transformers import SparseAutoModelForCausalLM, oneshot, apply
    import argparse
    from compressed_tensors.quantization import QuantizationScheme, QuantizationArgs, QuantizationType, QuantizationStrategy
    from llmcompressor.transformers.compression.helpers import calculate_offload_device_map
    import torch
    def parse_actorder(value):
        # Interpret the input value for --actorder
        if value.lower() == "false":
            return False
        elif value.lower() == "group":
            return "group"
        else:
            raise argparse.ArgumentTypeError("Invalid value for --actorder. Use 'group' or 'False'.")
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_path', type=str)
    parser.add_argument('--quant_path', type=str)
    parser.add_argument('--num_bits', type=int, default=4)
    parser.add_argument('--sequential_update', type=bool, default=True)
    parser.add_argument('--calib_size', type=int, default=256) 
    parser.add_argument('--dampening_frac', type=float, default=0.05)
    parser.add_argument('--observer', type=str, default="minmax") 
    parser.add_argument(
        '--actorder',
        type=parse_actorder,
        default=False,  # Default value is False
        help="Specify actorder as 'group' (string) or False (boolean)."
    )
    args = parser.parse_args()
    device_map = calculate_offload_device_map(
        args.model_path,
        reserve_for_hessians=True,
        num_gpus=torch.cuda.device_count(),
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    )
    model = SparseAutoModelForCausalLM.from_pretrained(
        args.model_path,
        device_map=device_map,
        torch_dtype=torch.bfloat16,
        use_cache=False,
    )
    tokenizer = AutoTokenizer.from_pretrained(args.model_path)
    NUM_CALIBRATION_SAMPLES = args.calib_size
    DATASET_ID = "garage-bAInd/Open-Platypus"
    DATASET_SPLIT = "train"
    ds = load_dataset(DATASET_ID, split=DATASET_SPLIT)
    ds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))
    def preprocess(example):
        concat_txt = example["instruction"] + "\n" + example["output"]
        return {"text": concat_txt}
    ds = ds.map(preprocess)
    def tokenize(sample):
        return tokenizer(
            sample["text"],
            padding=False,
            truncation=False,
            add_special_tokens=True,
        )
    ds = ds.map(tokenize, remove_columns=ds.column_names)
    quant_scheme = QuantizationScheme(
        targets=["Linear"],
        weights=QuantizationArgs(
            num_bits=args.num_bits,
            type=QuantizationType.INT,
            symmetric=True,
            group_size=128,
            strategy=QuantizationStrategy.GROUP,  
            observer=args.observer,
            actorder=args.actorder
        ),
        input_activations=None,
        output_activations=None,
    )
    recipe = [
        GPTQModifier(
            targets=["Linear"],
            ignore=["lm_head", "re:.*block_sparse_moe.gate"],
            sequential_update=args.sequential_update,
            dampening_frac=args.dampening_frac,
            config_groups={"group_0": quant_scheme},
        )
    ]
    oneshot(
        model=model,
        dataset=ds,
        recipe=recipe,
        num_calibration_samples=args.calib_size,
    )
    # Save to disk compressed.
    SAVE_DIR = args.quant_path
    model.save_pretrained(SAVE_DIR, save_compressed=True)
    tokenizer.save_pretrained(SAVE_DIR)
    ```

    ## Evaluation

    The model was evaluated on OpenLLM Leaderboard [V1](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard) using the following command:

    OpenLLM Leaderboard V1:
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="neuralmagic-ent/Mixtral-8x22B-v0.1-quantized.w4a16",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=4,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \
      --tasks openllm \
      --write_out \
      --batch_size auto \
      --output_path output_dir \
      --show_config
    ```


    ### Accuracy

    #### OpenLLM Leaderboard V1 evaluation scores

    | Metric                                   | mistralai/Mixtral-8x22B-v0.1             | neuralmagic-ent/Mixtral-8x22B-v0.1-quantized.w4a16 |
    |-----------------------------------------|:---------------------------------:|:-------------------------------------------:|
    | ARC-Challenge (Acc-Norm, 25-shot)       | 70.39                            | 69.88                                       |
    | GSM8K (Strict-Match, 5-shot)            | 76.42                            | 74.68                                        |
    | HellaSwag (Acc-Norm, 10-shot)           | 88.31                            | 87.94                                       |
    | MMLU (Acc, 5-shot)                      | 77.40                            | 76.21                                       |
    | TruthfulQA (MC2, 0-shot)                | 51.17                            | 51.15                                       |
    | Winogrande (Acc, 5-shot)                | 84.45                            | 85.16                                       |
    | **Average Score**                       | **74.69**                        | **74.17**                                   |
    | **Recovery**                            | **100.00**                       | **99.30**                                   |
  logo: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAACYktHRAAAqo0jMgAAAAlwSFlzAAAAVQAAAFUABu0WZwAAAAd0SU1FB+gMExEDHn1ZgdgAAAABb3JOVAHPoneaAAACbUlEQVRIx93UXUiedRgG8N98nRBN5zrwIDZLpXStT0XWu8JkYM1RLGgfUZQydhA7E+rEYmw5txMN7CDCMSPYYYyCwLCTGQQ528FUNmhK4mt6YH6kNuZcvTvYv2fvl+ZpXSfPdd//+76em+u5/w//W1T73k3j2pw27qY+z228ucg+i06q0mDYsAZVTvnDPkX/3lzgjNsWfBjiTp2B9Vm1ol1BekN+hsCXKtW6igfU6w3ZNzSr9qQH9Sj1ztrvf8ayHYE/blEsTHBCm+1gh2VPrT1B3KBE4GN+0yrP3/g4qki4LG54rQmaDKRETxt3O/LgH9RGM2ZhtwlfZTh0IUsgA3kRe8S3OhwEMaOKccev/kyrbzVnzGu5pM7riXiD2SCdL4ZH1ahR410zXvCWxfsbsSl6/u5T10CF97XqjuSKTZgBWwxpEDMnHmojlEi64mejkoY1pp19oD+wUgu6XDSStT8K/BXcPWbalpSTzSYciKJnfe6sklwe9GsJ7Ib6wMp1+cYvKVavg1cs2AVe8hDYasp5PaZsTal703fOeTiXRKsFpWmS94y67uUoV2/OcRcM5hIoc0txSlxhSaP9lpSjyoqkpG4USaZNFfCqocBiKuWj2aRJTSBuWrkmU17X4XquCV40Fr72T+bS7xzi4Zq1GNFrZy6BbW6pQItLNmedxqN7mobUDzTvC5+IKTNgNeSetzvU1SlUvb4AHyn1te2SUeaQw6DTMRddUre+wKy9EvbncPioI476LPt3lrlj847b622PZeQT6hSrze1DNtpNe88T9vhBB9hjUlKfwo0JcMiPliSci3Zz08ab/2O4Cy0mnnC8jSUfAAAAAElFTkSuQmCC
  language: ["en"]
  license: apache-2.0
  licenseLink: https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md
  maturity: Generally Available
  libraryName: transformers
  baseModel:
    - repository: mistralai
    - name: Mixtral-8x22B-v0.1
  labels:
    - inference
  tasks:
    - text-generation
  createTimeSinceEpoch: 1735862400
  lastUpdateTimeSinceEpoch: 1735862400
  artifacts:
    - protocol: NOT YET IN REGISTRY
      createTimeSinceEpoch: 
      tags: 
      uri: https://huggingface.co/neuralmagic-ent/Mixtral-8x22B-v0.1-quantized.w4a16/commits/main
- repository: neuralmagic
  name: Pixtral-Large-Instruct-2411-hf-quantized.w8a8
  provider: Neural Magic
  description: Quantized version of neuralmagic/Pixtral-Large-Instruct-2411-hf.
  longDescription: |-
    This model was obtained by quantizing the weights of neuralmagic/Pixtral-Large-Instruct-2411-hf to 
      INT8 data type, ready for inference with vLLM >= 0.5.2.
  readme: |-
    # Pixtral-Large-Instruct-2411-hf-quantized.w8a8

    ## Model Overview
    - **Model Architecture:** neuralmagic/Pixtral-Large-Instruct-2411-hf
      - **Input:** Vision-Text
      - **Output:** Text
    - **Model Optimizations:**
      - **Weight quantization:** INT8
      - **Activation quantization:** INT8
    - **Release Date:** 2/24/2025
    - **Version:** 1.0
    - **Model Developers:** Neural Magic

    Quantized version of [neuralmagic/Pixtral-Large-Instruct-2411-hf](https://huggingface.co/neuralmagic/Pixtral-Large-Instruct-2411-hf/tree/main).

    ### Model Optimizations

    This model was obtained by quantizing the weights of [neuralmagic/Pixtral-Large-Instruct-2411-hf](https://huggingface.co/neuralmagic/Pixtral-Large-Instruct-2411-hf/tree/main) to INT8 data type, ready for inference with vLLM >= 0.5.2.

    ## Deployment

    ### Use with vLLM

    This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

    ```python
    from vllm.assets.image import ImageAsset
    from vllm import LLM, SamplingParams
    # prepare model
    llm = LLM(
        model="neuralmagic/Pixtral-Large-Instruct-2411-hf-quantized.w8a8",
        trust_remote_code=True,
        max_model_len=4096,
        max_num_seqs=2,
    )
    # prepare inputs
    question = "What is the content of this image?"
    inputs = {
        "prompt": f"<|user|>\n<|image_1|>\n{question}<|end|>\n<|assistant|>\n",
        "multi_modal_data": {
            "image": ImageAsset("cherry_blossom").pil_image.convert("RGB")
        },
    }
    # generate response
    print("========== SAMPLE GENERATION ==============")
    outputs = llm.generate(inputs, SamplingParams(temperature=0.2, max_tokens=64))
    print(f"PROMPT  : {outputs[0].prompt}")
    print(f"RESPONSE: {outputs[0].outputs[0].text}")
    print("==========================================")
    ```

    vLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

    ## Creation

    This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below as part a multimodal announcement blog.

    <details>
      <summary>Model Creation Code</summary>
      
    ```python
    import requests
    import torch
    from PIL import Image
    from transformers import AutoProcessor
    from llmcompressor.modifiers.quantization import GPTQModifier
    from llmcompressor.transformers import oneshot
    from llmcompressor.transformers.tracing import TraceableLlavaForConditionalGeneration
            
    # Load model.
    model_id = "neuralmagic/Pixtral-Large-Instruct-2411-hf"
    model = TraceableLlavaForConditionalGeneration.from_pretrained(
        model_id, device_map="auto", torch_dtype="auto"
    )
    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
    # Oneshot arguments
    DATASET_ID = "flickr30k"
    DATASET_SPLIT = {"calibration": "test[:512]"}
    NUM_CALIBRATION_SAMPLES = 512
    MAX_SEQUENCE_LENGTH = 2048
    # Define a oneshot data collator for multimodal inputs.
    def data_collator(batch):
        assert len(batch) == 1
        return {
            "input_ids": torch.LongTensor(batch[0]["input_ids"]),
            "attention_mask": torch.tensor(batch[0]["attention_mask"]),
            "pixel_values": torch.tensor(batch[0]["pixel_values"]),
        }
    # Recipe
    recipe = [
        GPTQModifier(
            targets="Linear",
            scheme="W8A8",
            sequential_targets=["MistralDecoderLayer"],
            ignore=["re:.*lm_head", "re:vision_tower.*", "re:multi_modal_projector.*"],
        ),
    ]
    SAVE_DIR==f"{model_id.split('/')[1]}-quantized.w8a8"
    # Perform oneshot
    oneshot(
        model=model,
        tokenizer=model_id,
        dataset=DATASET_ID,
        splits=DATASET_SPLIT,
        recipe=recipe,
        max_seq_length=MAX_SEQUENCE_LENGTH,
        num_calibration_samples=NUM_CALIBRATION_SAMPLES,
        trust_remote_code_model=True,
        data_collator=data_collator,
        output_dir=SAVE_DIR
    )
    ```
    </details>

    ## Evaluation

    The model was evaluated using [mistral-evals](https://github.com/neuralmagic/mistral-evals) for vision-related tasks and using [lm_evaluation_harness](https://github.com/neuralmagic/lm-evaluation-harness) for select text-based benchmarks. The evaluations were conducted using the following commands:

    <details>
    <summary>Evaluation Commands</summary>
      
    ### Vision Tasks
    - vqav2
    - docvqa
    - mathvista
    - mmmu
    - chartqa

    ```
    vllm serve neuralmagic/pixtral-12b-quantized.w8a8 --tensor_parallel_size 1 --max_model_len 25000 --trust_remote_code --max_num_seqs 8 --gpu_memory_utilization 0.9 --dtype float16 --limit_mm_per_prompt image=7
    python -m eval.run eval_vllm \
            --model_name neuralmagic/pixtral-12b-quantized.w8a8 \
            --url http://0.0.0.0:8000 \
            --output_dir ~/tmp \
            --eval_name <vision_task_name>
    ```

    ### Text-based Tasks
    #### MMLU
      
    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="<model_name>",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=<n>,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \
      --tasks mmlu \
      --num_fewshot 5 \
      --batch_size auto \
      --output_path output_dir
    ```

    #### MGSM

    ```
    lm_eval \
      --model vllm \
      --model_args pretrained="<model_name>",dtype=auto,max_model_len=4096,max_gen_toks=2048,max_num_seqs=128,tensor_parallel_size=<n>,gpu_memory_utilization=0.9 \
      --tasks mgsm_cot_native \
      --num_fewshot 0 \
      --batch_size auto \
      --output_path output_dir
    ```
    </details>


    ### Accuracy

    <table>
      <thead>
        <tr>
          <th>Category</th>
          <th>Metric</th>
          <th>neuralmagic/Pixtral-Large-Instruct-2411-hf</th>
          <th>neuralmagic/Pixtral-Large-Instruct-2411-hf-quantized.w8a8</th>
          <th>Recovery (%)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td rowspan="6"><b>Vision</b></td>
          <td>MMMU (val, CoT)<br><i>explicit_prompt_relaxed_correctness</i></td>
          <td>63.56</td>
          <td>63.89</td>
          <td>100.52%</td>
        </tr>
        <tr>
          <td>VQAv2 (val)<br><i>vqa_match</i></td>
          <td>79.03</td>
          <td>79.12</td>
          <td>100.11%</td>
        </tr>
        <tr>
          <td>DocVQA (val)<br><i>anls</i></td>
          <td>89.55</td>
          <td>89.80</td>
          <td>100.28%</td>
        </tr>
        <tr>
          <td>ChartQA (test, CoT)<br><i>anywhere_in_answer_relaxed_correctness</i></td>
          <td>82.24</td>
          <td>80.44</td>
          <td>97.81%</td>
        </tr>
        <tr>
          <td>Mathvista (testmini, CoT)<br><i>explicit_prompt_relaxed_correctness</i></td>
          <td>67.3</td>
          <td>66.50</td>
          <td>98.81%</td>
        </tr>
        <tr>
          <td><b>Average Score</b></td>
          <td><b>76.34</b></td>
          <td><b>75.95</b></td>
          <td><b>99.49%</b></td>
        </tr>
        <tr>
          <td rowspan="2"><b>Text</b></td>
          <td>MGSM (CoT)</td>
          <td>76.05</td>
          <td>74.76</td>
          <td>98.30%</td>
        </tr>
        <tr>
          <td>MMLU (5-shot)</td>
          <td>82.8</td>
          <td>82.9</td>
          <td>100.12%</td>
        </tr>
      </tbody>
    </table>

    ## Inference Performance


    This model achieves up to 1.87x speedup in single-stream deployment and up to 2.0x speedup in multi-stream asynchronous deployment, depending on hardware and use-case scenario.
    The following performance benchmarks were conducted with [vLLM](https://docs.vllm.ai/en/latest/) version 0.7.2, and [GuideLLM](https://github.com/neuralmagic/guidellm).

    <details>
    <summary>Benchmarking Command</summary>
    ```
      guidellm --model neuralmagic/Pixtral-Large-Instruct-2411-hf-quantized.w8a8 --target "http://localhost:8000/v1" --data-type emulated --data prompt_tokens=<prompt_tokens>,generated_tokens=<generated_tokens>,images=<num_images>,width=<image_width>,height=<image_height> --max seconds 120 --backend aiohttp_server
    ```

    </details>

    ### Single-stream performance (measured with vLLM version 0.7.2)

    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th style="text-align: center;" colspan="2" >Document Visual Question Answering<br>1680W x 2240H<br>64/128</th>
          <th style="text-align: center;" colspan="2" >Visual Reasoning <br>640W x 480H<br>128/128</th>
          <th style="text-align: center;" colspan="2" >Image Captioning<br>480W x 360H<br>0/128</th>
        </tr>
        <tr>
          <th>Hardware</th>
          <th>Number of GPUs</th>
          <th>Model</th>
          <th>Average Cost Reduction</th>
          <th>Latency (s)</th>
          <th>Queries Per Dollar</th>
          <th>Latency (s)</th>
          <th>Queries Per Dollar</th>
          <th>Latency (s)</th>
          <th>Queries Per Dollar</th>
        </tr>
      </thead>
      <tbody style="text-align: center">
        <tr>
          <th rowspan="3" valign="top">A100</th>
          <td>4</td>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf</td>
          <td></td>
          <td>7.5</td>
          <td>67</td>
          <td>6.5</td>
          <td>77</td>
          <td>6.4</td>
          <td>79</td>
        </tr>
        <tr>
          <td>2</td>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf-quantized.w8a8</td>
          <td>1.86</td>
          <td>8.1</td>
          <td>124</td>
          <td>7.1</td>
          <td>142</td>
          <td>6.8</td>
          <td>148</td>
        </tr>
        <tr>
          <td>2</td>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf-quantized.w4a16</td>
          <td>2.52</td>
          <td>6.9</td>
          <td>147</td>
          <td>5.1</td>
          <td>199</td>
          <td>4.5</td>
          <td>221</td>
        </tr>
        <tr>
          <th rowspan="3" valign="top">H100</th>
          <td>4</td>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf</td>
          <td></td>
          <td>4.4</td>
          <td>67</td>
          <td>3.9</td>
          <td>74</td>
          <td>3.7</td>
          <td>79</td>
        </tr>
        <tr>
          <td>2</td>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf-FP8-Dynamic</td>
          <td>1.82</td>
          <td>4.7</td>
          <td>120</td>
          <td>4.1</td>
          <td>137</td>
          <td>3.9</td>
          <td>145</td>
        </tr>
        <tr>
          <td>2</td>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf-quantized.w4a16</td>
          <td>1.87</td>
          <td>4.7</td>
          <td>120</td>
          <td>3.9</td>
          <td>144</td>
          <td>3.8</td>
          <td>149</td>
        </tr>
      </tbody>
    </table>
    
    **Use case profiles: Image Size (WxH) / prompt tokens / generation tokens
    **QPD: Queries per dollar, based on on-demand cost at [Lambda Labs](https://lambdalabs.com/service/gpu-cloud) (observed on 2/18/2025).
    ### Multi-stream asynchronous performance (measured with vLLM version 0.7.2)
    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th style="text-align: center;" colspan="2" >Document Visual Question Answering<br>1680W x 2240H<br>64/128</th>
          <th style="text-align: center;" colspan="2" >Visual Reasoning <br>640W x 480H<br>128/128</th>
          <th style="text-align: center;" colspan="2" >Image Captioning<br>480W x 360H<br>0/128</th>
        </tr>
        <tr>
          <th>Hardware</th>
          <th>Model</th>
          <th>Average Cost Reduction</th>
          <th>Maximum throughput (QPS)</th>
          <th>Queries Per Dollar</th>
          <th>Maximum throughput (QPS)</th>
          <th>Queries Per Dollar</th>
          <th>Maximum throughput (QPS)</th>
          <th>Queries Per Dollar</th>
        </tr>
      </thead>
      <tbody style="text-align: center">
      <tr>
          <th rowspan="3" valign="top">A100x4</th>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf</td>
          <td></td>
          <td>0.4</td>
          <td>222</td>
          <td>0.7</td>
          <td>341</td>
          <td>0.8</td>
          <td>399</td>
        </tr>
        <tr>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf-quantized.w8a8</td>
          <td>1.70</td>
          <td>0.8</td>
          <td>383</td>
          <td>1.1</td>
          <td>571</td>
          <td>1.3</td>
          <td>674</td>
        </tr>
        <tr>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf-quantized.w4a16</td>
          <td>1.48</td>
          <td>0.5</td>
          <td>276</td>
          <td>1.0</td>
          <td>505</td>
          <td>1.4</td>
          <td>680</td>
        </tr>
        <tr>
          <<th rowspan="3" valign="top">H100x4</th>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf</td>
          <td></td>
          <td>1.0</td>
          <td>284</td>
          <td>1.6</td>
          <td>465</td>
          <td>1.8</td>
          <td>511</td>
        </tr>
        <tr>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf-FP8-Dynamic</td>
          <td>1.61</td>
          <td>1.7</td>
          <td>467</td>
          <td>2.6</td>
          <td>726</td>
          <td>3.2</td>
          <td>908</td>
        </tr>
        <tr>
          <td>neuralmagic/Pixtral-Large-Instruct-2411-hf-quantized.w4a16</td>
          <td>1.33</td>
          <td>1.4</td>
          <td>393</td>
          <td>2.2</td>
          <td>726</td>
          <td>2.7</td>
          <td>764</td>
        </tr>
      </tbody>
    </table>
    **Use case profiles: Image Size (WxH) / prompt tokens / generation tokens

    **QPS: Queries per second.
    **QPD: Queries per dollar, based on on-demand cost at [Lambda Labs](https://lambdalabs.com/service/gpu-cloud) (observed on 2/18/2025).

    ## The Mistral AI Team

    Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickaël Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Théophile Gervet, Timothée Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall
  logo: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAACYktHRAAAqo0jMgAAAAlwSFlzAAAAVQAAAFUABu0WZwAAAAd0SU1FB+gMExEDHn1ZgdgAAAABb3JOVAHPoneaAAACbUlEQVRIx93UXUiedRgG8N98nRBN5zrwIDZLpXStT0XWu8JkYM1RLGgfUZQydhA7E+rEYmw5txMN7CDCMSPYYYyCwLCTGQQ528FUNmhK4mt6YH6kNuZcvTvYv2fvl+ZpXSfPdd//+76em+u5/w//W1T73k3j2pw27qY+z228ucg+i06q0mDYsAZVTvnDPkX/3lzgjNsWfBjiTp2B9Vm1ol1BekN+hsCXKtW6igfU6w3ZNzSr9qQH9Sj1ztrvf8ayHYE/blEsTHBCm+1gh2VPrT1B3KBE4GN+0yrP3/g4qki4LG54rQmaDKRETxt3O/LgH9RGM2ZhtwlfZTh0IUsgA3kRe8S3OhwEMaOKccev/kyrbzVnzGu5pM7riXiD2SCdL4ZH1ahR410zXvCWxfsbsSl6/u5T10CF97XqjuSKTZgBWwxpEDMnHmojlEi64mejkoY1pp19oD+wUgu6XDSStT8K/BXcPWbalpSTzSYciKJnfe6sklwe9GsJ7Ib6wMp1+cYvKVavg1cs2AVe8hDYasp5PaZsTal703fOeTiXRKsFpWmS94y67uUoV2/OcRcM5hIoc0txSlxhSaP9lpSjyoqkpG4USaZNFfCqocBiKuWj2aRJTSBuWrkmU17X4XquCV40Fr72T+bS7xzi4Zq1GNFrZy6BbW6pQItLNmedxqN7mobUDzTvC5+IKTNgNeSetzvU1SlUvb4AHyn1te2SUeaQw6DTMRddUre+wKy9EvbncPioI476LPt3lrlj847b622PZeQT6hSrze1DNtpNe88T9vhBB9hjUlKfwo0JcMiPliSci3Zz08ab/2O4Cy0mnnC8jSUfAAAAAElFTkSuQmCC
  language: ["en", "fr", "de", "es", "it", "pt", "zh", "ja", "ru", "ko"]
  license: mrl
  licenseLink: https://mistral.ai/licenses/MRL-0.1.md
  maturity: Generally Available
  libraryName: transformers
  baseModel:
    - repository: neuralmagic
    - name: Pixtral-Large-Instruct-2411-hf
  labels:
    - inference
    - w8a8
    - int8
    - vllm
    - vision
  tasks:
    - image-text-to-text
  createTimeSinceEpoch: 1738886400
  lastUpdateTimeSinceEpoch: 1741046400
  artifacts:
    - protocol: NOT YET IN REGISTRY
      createTimeSinceEpoch: 
      tags: 
      uri: https://huggingface.co/neuralmagic/Pixtral-Large-Instruct-2411-hf-quantized.w8a8
- repository: neuralmagic
  name: whisper-large-v2-W4A16-G128
  provider: Neural Magic
  description: Quantized version of openai/whisper-large-v2.
  longDescription: |-
    This model was obtained by quantizing the weights of openai/whisper-large-v2 to INT4 data type, 
      ready for inference with vLLM >= 0.5.2.
  readme: |-
    # whisper-large-v2-quantized.w4a16

    ## Model Overview
    - **Model Architecture:** whisper-large-v2
      - **Input:** Audio-Text
      - **Output:** Text
    - **Model Optimizations:**
      - **Weight quantization:** INT4
      - **Activation quantization:** FP16
    - **Release Date:** 1/31/2025
    - **Version:** 1.0
    - **Model Developers:** Neural Magic

    Quantized version of [openai/whisper-large-v2](https://huggingface.co/openai/whisper-large-v2).

    ### Model Optimizations

    This model was obtained by quantizing the weights of [openai/whisper-large-v2](https://huggingface.co/openai/whisper-large-v2) to INT4 data type, ready for inference with vLLM >= 0.5.2.

    ## Deployment

    ### Use with vLLM

    This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

    ```python
    from vllm.assets.audio import AudioAsset
    from vllm import LLM, SamplingParams
    # prepare model
    llm = LLM(
        model="neuralmagic/whisper-large-v2-W4A16-G128",
        max_model_len=448,
        max_num_seqs=400,
        limit_mm_per_prompt={"audio": 1},
    )
    # prepare inputs
    inputs = {  # Test explicit encoder/decoder prompt
        "encoder_prompt": {
            "prompt": "",
            "multi_modal_data": {
                "audio": AudioAsset("winning_call").audio_and_sample_rate,
            },
        },
        "decoder_prompt": "<|startoftranscript|>",
    }
    # generate response
    print("========== SAMPLE GENERATION ==============")
    outputs = llm.generate(inputs, SamplingParams(temperature=0.0, max_tokens=64))
    print(f"PROMPT  : {outputs[0].prompt}")
    print(f"RESPONSE: {outputs[0].outputs[0].text}")
    print("==========================================")
    ```

    vLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

    ## Creation

    This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below as part a multimodal announcement blog.

    ```python
    import torch
    from datasets import load_dataset
    from transformers import WhisperProcessor
    from llmcompressor.modifiers.quantization import GPTQModifier
    from llmcompressor.transformers import oneshot
    from llmcompressor.transformers.tracing import TraceableWhisperForConditionalGeneration
    # Select model and load it.
    model_id = "openai/whisper-large-v2"
    model = TraceableWhisperForConditionalGeneration.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype="auto",
    )
    processor = WhisperProcessor.from_pretrained(model_id)
    # Configure processor the dataset task.
    processor.tokenizer.set_prefix_tokens(language="en", task="transcribe")
    # Select calibration dataset.
    DATASET_ID = "MLCommons/peoples_speech"
    DATASET_SUBSET = "test"
    DATASET_SPLIT = "test"
    # Select number of samples. 512 samples is a good place to start.
    # Increasing the number of samples can improve accuracy.
    NUM_CALIBRATION_SAMPLES = 512
    MAX_SEQUENCE_LENGTH = 2048
    # Load dataset and preprocess.
    ds = load_dataset(
        DATASET_ID,
        DATASET_SUBSET,
        split=f"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]",
        trust_remote_code=True,
    )
    # Preprocess and Tokenize inputs.
    def preprocess_and_tokenize(example):
        audio = example["audio"]["array"]
        sampling_rate = example["audio"]["sampling_rate"]
        text = " " + example["text"].capitalize()
        audio_inputs = processor(
            audio=audio,
            sampling_rate=sampling_rate,
            return_tensors="pt",
        )
        text_inputs = processor(
            text=text,
            add_special_tokens=True,
            return_tensors="pt"
        )
        text_inputs["decoder_input_ids"] = text_inputs["input_ids"]
        del text_inputs["input_ids"]
        return dict(**audio_inputs, **text_inputs)
    ds = ds.map(preprocess_and_tokenize, remove_columns=ds.column_names)
    # Define a oneshot data collator for multimodal inputs.
    def data_collator(batch):
        assert len(batch) == 1
        return {key: torch.tensor(value) for key, value in batch[0].items()}
    # Recipe
    recipe = GPTQModifier(targets="Linear", scheme="W4A16", ignore=["lm_head"])
    # Apply algorithms.
    SAVE_DIR = model_id.split("/")[1] + "-W4A16-G128"
    oneshot(
        model=model,
        dataset=ds,
        recipe=recipe,
        max_seq_length=MAX_SEQUENCE_LENGTH,
        num_calibration_samples=NUM_CALIBRATION_SAMPLES,
        data_collator=data_collator,
        output_dir=SAVE_DIR,
    )
    ```


    ### BibTeX entry and citation info
    ```bibtex
    @misc{radford2022whisper,
      doi = {10.48550/ARXIV.2212.04356},
      url = {https://arxiv.org/abs/2212.04356},
      author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
      title = {Robust Speech Recognition via Large-Scale Weak Supervision},
      publisher = {arXiv},
      year = {2022},
      copyright = {arXiv.org perpetual, non-exclusive license}
    }   
  logo: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAACYktHRAAAqo0jMgAAAAlwSFlzAAAAVQAAAFUABu0WZwAAAAd0SU1FB+gMExEDHn1ZgdgAAAABb3JOVAHPoneaAAACbUlEQVRIx93UXUiedRgG8N98nRBN5zrwIDZLpXStT0XWu8JkYM1RLGgfUZQydhA7E+rEYmw5txMN7CDCMSPYYYyCwLCTGQQ528FUNmhK4mt6YH6kNuZcvTvYv2fvl+ZpXSfPdd//+76em+u5/w//W1T73k3j2pw27qY+z228ucg+i06q0mDYsAZVTvnDPkX/3lzgjNsWfBjiTp2B9Vm1ol1BekN+hsCXKtW6igfU6w3ZNzSr9qQH9Sj1ztrvf8ayHYE/blEsTHBCm+1gh2VPrT1B3KBE4GN+0yrP3/g4qki4LG54rQmaDKRETxt3O/LgH9RGM2ZhtwlfZTh0IUsgA3kRe8S3OhwEMaOKccev/kyrbzVnzGu5pM7riXiD2SCdL4ZH1ahR410zXvCWxfsbsSl6/u5T10CF97XqjuSKTZgBWwxpEDMnHmojlEi64mejkoY1pp19oD+wUgu6XDSStT8K/BXcPWbalpSTzSYciKJnfe6sklwe9GsJ7Ib6wMp1+cYvKVavg1cs2AVe8hDYasp5PaZsTal703fOeTiXRKsFpWmS94y67uUoV2/OcRcM5hIoc0txSlxhSaP9lpSjyoqkpG4USaZNFfCqocBiKuWj2aRJTSBuWrkmU17X4XquCV40Fr72T+bS7xzi4Zq1GNFrZy6BbW6pQItLNmedxqN7mobUDzTvC5+IKTNgNeSetzvU1SlUvb4AHyn1te2SUeaQw6DTMRddUre+wKy9EvbncPioI476LPt3lrlj847b622PZeQT6hSrze1DNtpNe88T9vhBB9hjUlKfwo0JcMiPliSci3Zz08ab/2O4Cy0mnnC8jSUfAAAAAElFTkSuQmCC
  language: ["en"]
  license: apache-2.0
  licenseLink: https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md
  maturity: Generally Available
  libraryName: transformers
  baseModel:
    - repository: openai
    - name: whisper-large-v2
  labels:
    - inference
    - w4a16
    - int4
    - vllm
    - audio
  tasks:
    - automatic-speech-recognition
  createTimeSinceEpoch: 1738281600
  lastUpdateTimeSinceEpoch: 1738281600
  artifacts:
    - protocol: NOT YET IN REGISTRY
      createTimeSinceEpoch: 
      tags: 
      uri: https://huggingface.co/neuralmagic/whisper-large-v2-W4A16-G128
